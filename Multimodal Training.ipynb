{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf0ca71a",
   "metadata": {
    "papermill": {
     "duration": 0.005301,
     "end_time": "2025-07-01T16:14:36.021349",
     "exception": false,
     "start_time": "2025-07-01T16:14:36.016048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Phase 1: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "687de524",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T16:14:36.032201Z",
     "iopub.status.busy": "2025-07-01T16:14:36.031721Z",
     "iopub.status.idle": "2025-07-01T16:14:36.054391Z",
     "shell.execute_reply": "2025-07-01T16:14:36.053800Z"
    },
    "papermill": {
     "duration": 0.030061,
     "end_time": "2025-07-01T16:14:36.055657",
     "exception": false,
     "start_time": "2025-07-01T16:14:36.025596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # =====================================\n",
    "# # VIETNAMESE & ENGLISH TEXT EXTRACTION\n",
    "# # GUARANTEED 4723 SAMPLES OUTPUT\n",
    "# # =====================================\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# import os\n",
    "# os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# !pip install -q transformers>=4.52.0 accelerate>=1.0.0\n",
    "# !pip install -q av librosa easyocr datasets pandas scipy soundfile resampy\n",
    "\n",
    "# import av\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import pathlib\n",
    "# import gc\n",
    "# import easyocr\n",
    "# import pandas as pd\n",
    "# import time\n",
    "# import random\n",
    "# import re\n",
    "# from transformers import pipeline\n",
    "# from PIL import Image\n",
    "# import librosa\n",
    "\n",
    "# # Suppress logging\n",
    "# import transformers\n",
    "# transformers.logging.set_verbosity_error()\n",
    "# import logging\n",
    "# logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "# print(f\"COMPLETE VI+EN EXTRACTION - Device: {device}\")\n",
    "\n",
    "# class Config:\n",
    "#     # Dataset paths\n",
    "#     dataset_train_path = \"/kaggle/input/tikharm-dataset/Dataset/train\"\n",
    "#     dataset_val_path = \"/kaggle/input/tikharm-dataset/Dataset/val\"\n",
    "#     dataset_test_path = \"/kaggle/input/tikharm-dataset/Dataset/test\"\n",
    "#     extra_dataset_path = \"/kaggle/working/extra_tikharm\"\n",
    "    \n",
    "#     output_file = \"./tikharm_vi_en_complete.csv\"\n",
    "#     extra_train_ratio = 0.85\n",
    "    \n",
    "#     # Processing settings\n",
    "#     testing_mode = False\n",
    "#     samples_per_class = None\n",
    "    \n",
    "#     # Audio settings - 60s limit\n",
    "#     max_audio_duration = 60\n",
    "#     min_audio_duration = 0.5\n",
    "#     target_sample_rate = 16000\n",
    "#     audio_chunk_size = 30\n",
    "    \n",
    "#     # OCR settings\n",
    "#     enable_ocr = True\n",
    "#     ocr_frames = 2\n",
    "#     ocr_positions = [0.3, 0.7]\n",
    "#     ocr_max_size = 640\n",
    "    \n",
    "#     # Processing\n",
    "#     cleanup_interval = 15\n",
    "#     progress_interval = 25\n",
    "    \n",
    "#     # ENSURE COMPLETE PROCESSING\n",
    "#     save_all_samples = True  # Save all samples regardless of text extraction success\n",
    "#     continue_on_error = True  # Continue processing even if individual videos fail\n",
    "\n",
    "# config = Config()\n",
    "\n",
    "# class AdvancedSpamFilter:\n",
    "#     \"\"\"Simplified spam filter - only removes clear spam/bias, keeps all real content\"\"\"\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         # ONLY CLEAR SPAM/BIAS PATTERNS - very specific\n",
    "#         self.spam_patterns = [\n",
    "#             # Vietnamese subscription calls - all variations\n",
    "#             re.compile(r'.*hãy\\s+(đăng\\s*ký|subscribe).*kênh.*', re.IGNORECASE),\n",
    "#             re.compile(r'.*(đăng\\s*ký|subscribe).*kênh.*', re.IGNORECASE),\n",
    "#             re.compile(r'.*subscribe.*cho.*kênh.*', re.IGNORECASE),\n",
    "#             re.compile(r'.*theo\\s*dõi.*kênh.*', re.IGNORECASE),\n",
    "#             re.compile(r'.*nhấn.*đăng\\s*ký.*kênh.*', re.IGNORECASE),\n",
    "#             re.compile(r'.*bấm.*đăng\\s*ký.*kênh.*', re.IGNORECASE),\n",
    "            \n",
    "#             # English subscription calls - MUST contain channel/subscribe together\n",
    "#             re.compile(r'.*please\\s+(subscribe|follow).*channel.*', re.IGNORECASE),\n",
    "#             re.compile(r'.*(subscribe|follow).*channel.*please.*', re.IGNORECASE),\n",
    "#             re.compile(r'.*hit.*subscribe.*channel.*', re.IGNORECASE),\n",
    "#             re.compile(r'.*click.*subscribe.*channel.*', re.IGNORECASE),\n",
    "            \n",
    "#             # Vietnamese engagement spam - must be complete promotional phrases\n",
    "#             re.compile(r'.*bấm\\s+(like|chuông).*kênh.*', re.IGNORECASE),\n",
    "#             re.compile(r'.*bấm.*chuông.*thông.*báo.*kênh.*', re.IGNORECASE),\n",
    "#             re.compile(r'.*để\\s+không\\s+bỏ\\s+lỡ.*video.*', re.IGNORECASE),\n",
    "#             re.compile(r'.*(like|chia\\s*sẻ).*video.*kênh.*', re.IGNORECASE),\n",
    "            \n",
    "#             # English engagement spam - must be complete promotional phrases\n",
    "#             re.compile(r'.*(like|share).*video.*channel.*', re.IGNORECASE),\n",
    "#             re.compile(r'.*hit.*notification.*bell.*', re.IGNORECASE),\n",
    "#             re.compile(r'.*don\\'t\\s+forget.*subscribe.*', re.IGNORECASE),\n",
    "#             re.compile(r'.*smash.*like.*subscribe.*', re.IGNORECASE),\n",
    "            \n",
    "#             # Brand/Channel specific spam\n",
    "#             re.compile(r'.*ghiền\\s+mì\\s+gõ.*kênh.*', re.IGNORECASE),\n",
    "            \n",
    "#             # Social media platform calls - must be promotional\n",
    "#             re.compile(r'.*(follow|theo\\s*dõi).*(facebook|instagram|tiktok).*page.*', re.IGNORECASE),\n",
    "            \n",
    "#             # Generic promotional calls - must have multiple elements\n",
    "#             re.compile(r'.*subscribe.*and.*like.*channel.*', re.IGNORECASE),\n",
    "#             re.compile(r'.*like.*and.*subscribe.*channel.*', re.IGNORECASE),\n",
    "#             re.compile(r'.*support.*our.*channel.*', re.IGNORECASE),\n",
    "#             re.compile(r'.*ủng\\s*hộ.*kênh.*của.*', re.IGNORECASE),\n",
    "            \n",
    "#             # Pure special characters only\n",
    "#             re.compile(r'^[@#$%^&*()_+=\\[\\]{}|\\\\:\";\\'<>?,./\\s]+$'),\n",
    "            \n",
    "#             # Pure numbers only  \n",
    "#             re.compile(r'^[0-9\\s\\-\\.]+$'),\n",
    "#         ]\n",
    "        \n",
    "#         # ONLY EXTREME GIBBERISH - very conservative\n",
    "#         self.extreme_gibberish_patterns = [\n",
    "#             # 10+ consecutive repeated characters (very conservative)\n",
    "#             re.compile(r'(.)\\1{9,}'),\n",
    "            \n",
    "#             # 15+ consecutive Vietnamese diacritics (extremely rare)\n",
    "#             re.compile(r'[àáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđ]{15,}'),\n",
    "#         ]\n",
    "    \n",
    "#     def is_spam(self, text):\n",
    "#         \"\"\"Only detect clear spam/bias patterns\"\"\"\n",
    "#         if not text or len(text.strip()) < 1:\n",
    "#             return True\n",
    "        \n",
    "#         text_clean = text.lower().strip()\n",
    "        \n",
    "#         # Check for clear spam patterns\n",
    "#         for pattern in self.spam_patterns:\n",
    "#             if pattern.search(text_clean):\n",
    "#                 return True\n",
    "        \n",
    "#         # Check for extreme gibberish only\n",
    "#         for pattern in self.extreme_gibberish_patterns:\n",
    "#             if pattern.search(text):\n",
    "#                 return True\n",
    "        \n",
    "#         return False\n",
    "    \n",
    "#     def clean_repetition(self, text):\n",
    "#         \"\"\"Very conservative repetition cleaning\"\"\"\n",
    "#         if not text:\n",
    "#             return \"\"\n",
    "        \n",
    "#         # Only remove very excessive repetition (8+ times)\n",
    "#         cleaned = re.sub(r'\\b(\\w+)(\\s+\\1){7,}\\b', r'\\1', text)\n",
    "        \n",
    "#         # Clean excessive spaces only\n",
    "#         cleaned = re.sub(r'\\s{8,}', ' ', cleaned)  # 8+ spaces -> 1\n",
    "#         cleaned = cleaned.strip()\n",
    "        \n",
    "#         return cleaned\n",
    "    \n",
    "#     def process_text(self, text):\n",
    "#         \"\"\"Minimal processing - only remove clear spam\"\"\"\n",
    "#         if not text:\n",
    "#             return \"\"\n",
    "        \n",
    "#         # Basic cleaning\n",
    "#         cleaned = text.strip()\n",
    "#         cleaned = re.sub(r'\\s+', ' ', cleaned)  # Multiple spaces to single\n",
    "        \n",
    "#         # Clean repetition very conservatively first\n",
    "#         cleaned = self.clean_repetition(cleaned)\n",
    "        \n",
    "#         # Check for spam only AFTER cleaning\n",
    "#         if self.is_spam(cleaned):\n",
    "#             return \"\"\n",
    "        \n",
    "#         # NO minimum length requirement - keep all non-spam text\n",
    "#         return cleaned\n",
    "\n",
    "# class WhisperHallucinationFilter:\n",
    "#     \"\"\"Filter để loại bỏ Whisper hallucination patterns\"\"\"\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         # Common Whisper hallucination patterns\n",
    "#         self.hallucination_patterns = [\n",
    "#             # Exact repetitive phrases - very specific\n",
    "#             re.compile(r'^thank\\s+you\\.?$', re.IGNORECASE),\n",
    "#             re.compile(r'^oh\\s+thank\\s+you\\.?$', re.IGNORECASE),\n",
    "#             re.compile(r'^thank\\s+you\\.?\\s+so\\s+so\\.?$', re.IGNORECASE),\n",
    "#             re.compile(r'^thank\\s+you\\.?\\s+you\\.?$', re.IGNORECASE),\n",
    "            \n",
    "#             # Single word hallucinations\n",
    "#             re.compile(r'^oh\\.?$', re.IGNORECASE),\n",
    "#             re.compile(r'^you\\.?$', re.IGNORECASE),\n",
    "#             re.compile(r'^bye\\.?$', re.IGNORECASE),\n",
    "#             re.compile(r'^hello\\.?$', re.IGNORECASE),\n",
    "#             re.compile(r'^hi\\.?$', re.IGNORECASE),\n",
    "            \n",
    "#             # Repetitive short phrases\n",
    "#             re.compile(r'^(oh\\s+){2,}.*', re.IGNORECASE),\n",
    "#             re.compile(r'^(you\\s+){2,}.*', re.IGNORECASE),\n",
    "#             re.compile(r'^(hey\\s+){2,}.*', re.IGNORECASE),\n",
    "            \n",
    "#             # Very generic phrases that appear too often with low confidence\n",
    "#             re.compile(r'^i\\s+just\\s+wanna\\s+be\\s+with\\s+you\\.?$', re.IGNORECASE),\n",
    "#             re.compile(r'^everybody\\s+make\\s+some\\s+noise\\.?$', re.IGNORECASE),\n",
    "#             re.compile(r'^everybody\\s+make\\s+some\\s+noise\\s+foreign\\.?$', re.IGNORECASE),\n",
    "            \n",
    "#             # Very short generic greetings without context\n",
    "#             re.compile(r'^(oh\\s+)?(bye|hello|hi)\\.?$', re.IGNORECASE),\n",
    "#         ]\n",
    "        \n",
    "#         # Confidence-based filtering for very short phrases\n",
    "#         self.low_confidence_short_phrases = [\n",
    "#             'thank you', 'oh', 'you', 'bye', 'hello', 'hi', 'hey'\n",
    "#         ]\n",
    "        \n",
    "#         # Phrases that are OK if they're part of longer sentences\n",
    "#         self.context_dependent_phrases = [\n",
    "#             'thank you', 'oh', 'you', 'bye', 'hello', 'hi'\n",
    "#         ]\n",
    "    \n",
    "#     def is_likely_hallucination(self, text):\n",
    "#         \"\"\"Check if text is likely Whisper hallucination\"\"\"\n",
    "#         if not text or len(text.strip()) < 1:\n",
    "#             return True\n",
    "        \n",
    "#         text_clean = text.strip().lower()\n",
    "#         word_count = len(text_clean.split())\n",
    "        \n",
    "#         # Check exact hallucination patterns\n",
    "#         for pattern in self.hallucination_patterns:\n",
    "#             if pattern.match(text_clean):\n",
    "#                 return True\n",
    "        \n",
    "#         # Check for very short generic phrases (1-2 words)\n",
    "#         if word_count <= 2:\n",
    "#             for phrase in self.low_confidence_short_phrases:\n",
    "#                 # If the entire text is just this phrase (with minimal variation)\n",
    "#                 if phrase in text_clean and len(text_clean) <= len(phrase) + 3:\n",
    "#                     return True\n",
    "        \n",
    "#         # Check for standalone context-dependent phrases\n",
    "#         if word_count == 1:\n",
    "#             if text_clean.rstrip('.!?') in ['oh', 'you', 'bye', 'hi', 'hey']:\n",
    "#                 return True\n",
    "        \n",
    "#         return False\n",
    "    \n",
    "#     def filter_text(self, text):\n",
    "#         \"\"\"Filter out hallucinations but keep real content\"\"\"\n",
    "#         if not text:\n",
    "#             return \"\"\n",
    "        \n",
    "#         if self.is_likely_hallucination(text):\n",
    "#             return \"\"\n",
    "        \n",
    "#         return text\n",
    "\n",
    "# class FastLanguageDetector:\n",
    "#     \"\"\"Fast language detection - only Vietnamese and English\"\"\"\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def detect_language(text):\n",
    "#         if not text or len(text.strip()) < 1:\n",
    "#             return \"other\"\n",
    "        \n",
    "#         text_lower = text.lower()\n",
    "        \n",
    "#         vi_chars = len(re.findall(r'[àáạảãâầấậẩẫăằắặẳẵèéẹẻẽêềếệểễìíịỉĩòóọỏõôồốộổỗơờớợởỡùúụủũưừứựửữỳýỵỷỹđ]', text_lower))\n",
    "#         vi_words = len(re.findall(r'\\b(và|của|trong|với|từ|để|này|đó|có|là|được|một|tôi|bạn|anh|em|không|rất|cảm|ơn|tình|yêu|xin|chào|cám|chúc|mừng|được|làm|đi|về|gì|như|nào|khi|nếu|bây|giờ|hôm|nay|ngày|mai)\\b', text_lower))\n",
    "        \n",
    "#         en_words = len(re.findall(r'\\b(the|and|for|are|but|not|you|all|can|this|that|with|from|they|have|love|see|now|time|thank|hello|yes|no|good|great|nice|well|very|much|more|come|go|know|want|need|help|please|sorry|ok|okay|what|when|where|why|how|who|will|would|could|should|may|might|must|do|does|did|am|is|was|were|been|being|has|had|having|get|got|give|take|make|made|say|said|tell|told|ask|think|feel|look|seems|try|work|play|use|used|like|want|need)\\b', text_lower))\n",
    "        \n",
    "#         other_asian = len(re.findall(r'[\\u0E00-\\u0E7F\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FAF]', text))\n",
    "        \n",
    "#         # Reject other Asian languages\n",
    "#         if other_asian > len(text) * 0.1:\n",
    "#             return \"other\"\n",
    "        \n",
    "#         # Scoring system\n",
    "#         vi_score = vi_chars * 3 + vi_words * 4\n",
    "#         en_score = en_words * 3\n",
    "        \n",
    "#         # For very short text (1-3 words), be more specific\n",
    "#         word_count = len(text.split())\n",
    "#         if word_count <= 3:\n",
    "#             # If has Vietnamese characters or clear Vietnamese words\n",
    "#             if vi_chars > 0:\n",
    "#                 return \"vietnamese\"\n",
    "#             elif vi_words > 0:\n",
    "#                 return \"vietnamese\"\n",
    "#             # If has clear English words\n",
    "#             elif en_words > 0:\n",
    "#                 return \"english\"\n",
    "#             # If it's basic English words that might not be in the regex\n",
    "#             elif any(word in text_lower for word in ['thank', 'hello', 'yes', 'no', 'ok', 'good', 'bad', 'nice', 'great', 'wow', 'ah', 'um', 'hmm']):\n",
    "#                 return \"english\"\n",
    "#             else:\n",
    "#                 return \"other\"\n",
    "        \n",
    "#         # For longer text, use scores\n",
    "#         if vi_score > en_score and vi_score > 0:\n",
    "#             return \"vietnamese\"\n",
    "#         elif en_score > 0:\n",
    "#             return \"english\"\n",
    "#         elif vi_chars > 0:  # Has Vietnamese diacritics\n",
    "#             return \"vietnamese\"\n",
    "#         else:\n",
    "#             return \"other\"\n",
    "    \n",
    "#     @staticmethod\n",
    "#     def is_target_language(text):\n",
    "#         detected = FastLanguageDetector.detect_language(text)\n",
    "#         # ONLY accept Vietnamese and English\n",
    "#         return detected in [\"vietnamese\", \"english\"]\n",
    "\n",
    "# class FastOCRExtractor:\n",
    "#     \"\"\"Fast OCR extraction with error handling\"\"\"\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         print(\"Loading Fast EasyOCR...\")\n",
    "#         try:\n",
    "#             self.reader = easyocr.Reader(['vi', 'en'], gpu=torch.cuda.is_available(), verbose=False)\n",
    "#             print(\"✓ OCR Ready\")\n",
    "#             self.available = True\n",
    "#         except Exception as e:\n",
    "#             print(f\"✗ OCR Failed: {e}\")\n",
    "#             self.reader = None\n",
    "#             self.available = False\n",
    "        \n",
    "#         self.spam_filter = AdvancedSpamFilter()\n",
    "    \n",
    "#     def extract_text_safe(self, video_path):\n",
    "#         \"\"\"Safe OCR extraction with comprehensive error handling\"\"\"\n",
    "#         if not self.available or not self.reader:\n",
    "#             return \"\"\n",
    "        \n",
    "#         try:\n",
    "#             container = av.open(str(video_path))\n",
    "#             if not container.streams.video:\n",
    "#                 container.close()\n",
    "#                 return \"\"\n",
    "            \n",
    "#             video_stream = container.streams.video[0]\n",
    "#             total_frames = max(video_stream.frames or 1000, 1000)\n",
    "#             frame_positions = [int(total_frames * pos) for pos in config.ocr_positions]\n",
    "            \n",
    "#             all_texts = []\n",
    "#             current_frame = 0\n",
    "#             extracted = 0\n",
    "            \n",
    "#             for frame in container.decode(video=0):\n",
    "#                 if current_frame in frame_positions:\n",
    "#                     try:\n",
    "#                         frame_array = frame.to_ndarray(format=\"rgb24\")\n",
    "                        \n",
    "#                         # Fast resize\n",
    "#                         h, w = frame_array.shape[:2]\n",
    "#                         if max(h, w) > config.ocr_max_size:\n",
    "#                             scale = config.ocr_max_size / max(h, w)\n",
    "#                             new_h, new_w = int(h * scale), int(w * scale)\n",
    "#                             img = Image.fromarray(frame_array)\n",
    "#                             img = img.resize((new_w, new_h), Image.LANCZOS)\n",
    "#                             frame_array = np.array(img)\n",
    "                        \n",
    "#                         # Safe OCR call\n",
    "#                         results = self.reader.readtext(\n",
    "#                             frame_array, \n",
    "#                             detail=0, \n",
    "#                             width_ths=0.7, \n",
    "#                             height_ths=0.7\n",
    "#                         )\n",
    "                        \n",
    "#                         for text in results:\n",
    "#                             if isinstance(text, str) and text.strip():\n",
    "#                                 processed = self.spam_filter.process_text(text.strip())\n",
    "#                                 if processed:\n",
    "#                                     # ONLY accept Vietnamese and English\n",
    "#                                     if FastLanguageDetector.is_target_language(processed):\n",
    "#                                         all_texts.append(processed)\n",
    "                        \n",
    "#                         extracted += 1\n",
    "#                     except Exception as ocr_error:\n",
    "#                         # Continue on individual frame errors\n",
    "#                         continue\n",
    "                \n",
    "#                 current_frame += 1\n",
    "#                 if extracted >= config.ocr_frames:\n",
    "#                     break\n",
    "            \n",
    "#             container.close()\n",
    "            \n",
    "#             if all_texts:\n",
    "#                 unique_texts = list(dict.fromkeys(all_texts))\n",
    "#                 combined = ' '.join(unique_texts)\n",
    "#                 final = self.spam_filter.process_text(combined)\n",
    "#                 # Final check for target languages\n",
    "#                 if final and FastLanguageDetector.is_target_language(final):\n",
    "#                     return final\n",
    "            \n",
    "#             return \"\"\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             # Return empty string on any error, don't crash\n",
    "#             return \"\"\n",
    "\n",
    "# class FastAudioExtractor:\n",
    "#     \"\"\"Fast audio extraction with comprehensive error handling\"\"\"\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         print(\"Loading Fast Whisper...\")\n",
    "#         try:\n",
    "#             self.pipe = pipeline(\n",
    "#                 \"automatic-speech-recognition\",\n",
    "#                 model=\"openai/whisper-large-v3-turbo\",\n",
    "#                 torch_dtype=torch_dtype,\n",
    "#                 device=device,\n",
    "#                 return_timestamps=False,\n",
    "#                 chunk_length_s=config.audio_chunk_size,\n",
    "#                 stride_length_s=5\n",
    "#             )\n",
    "#             print(\"✓ Whisper Ready\")\n",
    "#             self.available = True\n",
    "#         except Exception as e:\n",
    "#             print(f\"✗ Whisper Failed: {e}\")\n",
    "#             self.pipe = None\n",
    "#             self.available = False\n",
    "        \n",
    "#         self.spam_filter = AdvancedSpamFilter()\n",
    "#         self.hallucination_filter = WhisperHallucinationFilter()  # Thêm filter mới\n",
    "    \n",
    "#     def get_video_duration_safe(self, video_path):\n",
    "#         \"\"\"Safe video duration check\"\"\"\n",
    "#         try:\n",
    "#             container = av.open(str(video_path))\n",
    "#             if container.streams.video:\n",
    "#                 duration = float(container.duration) / av.time_base if container.duration else 0\n",
    "#                 container.close()\n",
    "#                 return min(duration, config.max_audio_duration)\n",
    "#             container.close()\n",
    "#             return 0\n",
    "#         except:\n",
    "#             return config.max_audio_duration\n",
    "    \n",
    "#     def extract_audio_safe(self, video_path):\n",
    "#         \"\"\"Safe audio extraction\"\"\"\n",
    "#         try:\n",
    "#             video_duration = self.get_video_duration_safe(video_path)\n",
    "#             duration_to_load = min(video_duration, config.max_audio_duration)\n",
    "            \n",
    "#             if duration_to_load < config.min_audio_duration:\n",
    "#                 return None, 0\n",
    "            \n",
    "#             audio, _ = librosa.load(\n",
    "#                 str(video_path),\n",
    "#                 sr=config.target_sample_rate,\n",
    "#                 duration=duration_to_load,\n",
    "#                 mono=True,\n",
    "#                 res_type='kaiser_fast'\n",
    "#             )\n",
    "            \n",
    "#             if len(audio) < config.min_audio_duration * config.target_sample_rate:\n",
    "#                 return None, 0\n",
    "            \n",
    "#             rms = np.sqrt(np.mean(audio**2))\n",
    "#             if rms < 1e-6:\n",
    "#                 return None, 0\n",
    "            \n",
    "#             max_val = np.abs(audio).max()\n",
    "#             if max_val > 0:\n",
    "#                 audio = audio / max_val * 0.8\n",
    "            \n",
    "#             actual_duration = len(audio) / config.target_sample_rate\n",
    "            \n",
    "#             return audio.astype(np.float32), actual_duration\n",
    "#         except:\n",
    "#             return None, 0\n",
    "    \n",
    "#     def transcribe_text_safe(self, video_path):\n",
    "#         \"\"\"Safe audio transcription with comprehensive error handling\"\"\"\n",
    "#         if not self.available or not self.pipe:\n",
    "#             return \"\"\n",
    "        \n",
    "#         try:\n",
    "#             audio, duration = self.extract_audio_safe(video_path)\n",
    "#             if audio is None:\n",
    "#                 return \"\"\n",
    "            \n",
    "#             best_result = \"\"\n",
    "            \n",
    "#             # Try Vietnamese first, then English\n",
    "#             for language in [\"vi\", \"en\"]:\n",
    "#                 try:\n",
    "#                     result = self.pipe(\n",
    "#                         audio,\n",
    "#                         generate_kwargs={\n",
    "#                             \"language\": language,\n",
    "#                             \"task\": \"transcribe\",\n",
    "#                             \"temperature\": 0.0,\n",
    "#                             \"no_speech_threshold\": 0.7,  # Tăng từ 0.6 để giảm hallucination\n",
    "#                             \"compression_ratio_threshold\": 2.4,\n",
    "#                             \"logprob_threshold\": -1.0,\n",
    "#                             \"no_repeat_ngram_size\": 3,\n",
    "#                             \"repetition_penalty\": 1.2  # Tăng từ 1.1\n",
    "#                         }\n",
    "#                     )\n",
    "                    \n",
    "#                     text = result.get('text', '').strip() if isinstance(result, dict) else str(result).strip()\n",
    "                    \n",
    "#                     if text:\n",
    "#                         # Kiểm tra hallucination TRƯỚC khi xử lý\n",
    "#                         if self.hallucination_filter.is_likely_hallucination(text):\n",
    "#                             continue  # Skip hallucination\n",
    "                        \n",
    "#                         processed = self.spam_filter.process_text(text)\n",
    "#                         if processed and len(processed) > len(best_result):\n",
    "#                             detected = FastLanguageDetector.detect_language(processed)\n",
    "#                             # ONLY accept Vietnamese and English\n",
    "#                             if detected in [\"vietnamese\", \"english\"]:\n",
    "#                                 # Final hallucination check\n",
    "#                                 final_filtered = self.hallucination_filter.filter_text(processed)\n",
    "#                                 if final_filtered:\n",
    "#                                     best_result = final_filtered\n",
    "#                                     # Prefer Vietnamese and longer text\n",
    "#                                     if detected == \"vietnamese\" and len(final_filtered) > 5:\n",
    "#                                         break\n",
    "#                 except Exception as whisper_error:\n",
    "#                     # Continue on individual language errors\n",
    "#                     continue\n",
    "            \n",
    "#             # Final validation\n",
    "#             if best_result and FastLanguageDetector.is_target_language(best_result):\n",
    "#                 return best_result\n",
    "#             else:\n",
    "#                 return \"\"\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             # Return empty string on any error, don't crash\n",
    "#             return \"\"\n",
    "\n",
    "# class CompleteProcessor:\n",
    "#     \"\"\"Complete processor that guarantees all samples are processed\"\"\"\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         self.ocr_extractor = FastOCRExtractor()\n",
    "#         self.audio_extractor = FastAudioExtractor()\n",
    "        \n",
    "#         # Comprehensive stats\n",
    "#         self.total_discovered = 0\n",
    "#         self.total_processed = 0\n",
    "#         self.total_completed = 0\n",
    "#         self.audio_success = 0\n",
    "#         self.ocr_success = 0\n",
    "#         self.combined_success = 0\n",
    "#         self.vietnamese_count = 0\n",
    "#         self.english_count = 0\n",
    "#         self.processing_errors = 0\n",
    "#         self.hallucination_filtered = 0  # Track filtered hallucinations\n",
    "        \n",
    "#         # Timing stats\n",
    "#         self.audio_time = 0\n",
    "#         self.ocr_time = 0\n",
    "        \n",
    "#         print(f\"Processor initialized:\")\n",
    "#         print(f\"  OCR available: {self.ocr_extractor.available}\")\n",
    "#         print(f\"  Audio available: {self.audio_extractor.available}\")\n",
    "#         print(f\"  Hallucination filter: Enabled\")\n",
    "    \n",
    "#     def process_video_safe(self, video_record):\n",
    "#         \"\"\"Safe video processing that always returns a record\"\"\"\n",
    "#         video_path = pathlib.Path(video_record['video_path'])\n",
    "#         video_name = video_path.name\n",
    "        \n",
    "#         self.total_processed += 1\n",
    "        \n",
    "#         # Initialize with empty values\n",
    "#         audio_text = \"\"\n",
    "#         ocr_text = \"\"\n",
    "#         processing_status = \"completed\"\n",
    "#         error_message = \"\"\n",
    "        \n",
    "#         try:\n",
    "#             # Extract audio with error handling\n",
    "#             if self.audio_extractor.available:\n",
    "#                 try:\n",
    "#                     audio_start = time.time()\n",
    "#                     audio_text = self.audio_extractor.transcribe_text_safe(video_path)\n",
    "#                     self.audio_time += time.time() - audio_start\n",
    "                    \n",
    "#                     if audio_text:\n",
    "#                         self.audio_success += 1\n",
    "#                         lang = FastLanguageDetector.detect_language(audio_text)\n",
    "#                         if lang == \"vietnamese\":\n",
    "#                             self.vietnamese_count += 1\n",
    "#                         elif lang == \"english\":\n",
    "#                             self.english_count += 1\n",
    "#                 except Exception as audio_error:\n",
    "#                     audio_text = \"\"\n",
    "#                     error_message += f\"Audio error: {str(audio_error)[:50]}; \"\n",
    "            \n",
    "#             # Extract OCR with error handling\n",
    "#             if self.ocr_extractor.available:\n",
    "#                 try:\n",
    "#                     ocr_start = time.time()\n",
    "#                     ocr_text = self.ocr_extractor.extract_text_safe(video_path)\n",
    "#                     self.ocr_time += time.time() - ocr_start\n",
    "                    \n",
    "#                     if ocr_text:\n",
    "#                         self.ocr_success += 1\n",
    "#                 except Exception as ocr_error:\n",
    "#                     ocr_text = \"\"\n",
    "#                     error_message += f\"OCR error: {str(ocr_error)[:50]}; \"\n",
    "            \n",
    "#         except Exception as general_error:\n",
    "#             processing_status = \"error\"\n",
    "#             error_message += f\"General error: {str(general_error)[:50]}; \"\n",
    "#             self.processing_errors += 1\n",
    "        \n",
    "#         # Combine results\n",
    "#         parts = []\n",
    "#         if audio_text:\n",
    "#             parts.append(f\"AUDIO: {audio_text}\")\n",
    "#         if ocr_text:\n",
    "#             parts.append(f\"OCR: {ocr_text}\")\n",
    "        \n",
    "#         combined = \" | \".join(parts) if parts else \"\"\n",
    "        \n",
    "#         if combined:\n",
    "#             self.combined_success += 1\n",
    "        \n",
    "#         # ALWAYS update record - this ensures we keep all 4723 samples\n",
    "#         video_record['ocr_text'] = ocr_text\n",
    "#         video_record['audio_text'] = audio_text\n",
    "#         video_record['combined_text'] = combined\n",
    "#         video_record['processing_status'] = processing_status\n",
    "#         video_record['error_message'] = error_message.strip()\n",
    "        \n",
    "#         self.total_completed += 1\n",
    "        \n",
    "#         # Print result\n",
    "#         if combined:\n",
    "#             print(f\"✓ {video_name}\")\n",
    "#             if audio_text:\n",
    "#                 print(f\"  Audio: {audio_text[:80]}{'...' if len(audio_text) > 80 else ''}\")\n",
    "#             if ocr_text:\n",
    "#                 print(f\"  OCR: {ocr_text[:80]}{'...' if len(ocr_text) > 80 else ''}\")\n",
    "#         elif processing_status == \"error\":\n",
    "#             print(f\"✗ {video_name} - Error: {error_message[:100]}\")\n",
    "#         else:\n",
    "#             print(f\"○ {video_name} - No valid text extracted\")\n",
    "        \n",
    "#         return video_record\n",
    "    \n",
    "#     def get_comprehensive_stats(self):\n",
    "#         return {\n",
    "#             'total_discovered': self.total_discovered,\n",
    "#             'total_processed': self.total_processed,\n",
    "#             'total_completed': self.total_completed,\n",
    "#             'audio_success': self.audio_success,\n",
    "#             'ocr_success': self.ocr_success,\n",
    "#             'combined_success': self.combined_success,\n",
    "#             'vietnamese_count': self.vietnamese_count,\n",
    "#             'english_count': self.english_count,\n",
    "#             'processing_errors': self.processing_errors,\n",
    "#             'hallucination_filtered': self.hallucination_filtered,\n",
    "#             'avg_audio_time': self.audio_time / max(self.total_processed, 1),\n",
    "#             'avg_ocr_time': self.ocr_time / max(self.total_processed, 1)\n",
    "#         }\n",
    "    \n",
    "#     def cleanup(self):\n",
    "#         gc.collect()\n",
    "#         if torch.cuda.is_available():\n",
    "#             torch.cuda.empty_cache()\n",
    "\n",
    "# def discover_all_videos():\n",
    "#     \"\"\"Complete video discovery with guaranteed count\"\"\"\n",
    "#     print(\"Discovering ALL videos...\")\n",
    "    \n",
    "#     records = []\n",
    "#     datasets = {\n",
    "#         'train': config.dataset_train_path,\n",
    "#         'val': config.dataset_val_path,\n",
    "#         'test': config.dataset_test_path\n",
    "#     }\n",
    "    \n",
    "#     # Check extra dataset\n",
    "#     if pathlib.Path(config.extra_dataset_path).exists():\n",
    "#         datasets['extra'] = config.extra_dataset_path\n",
    "#         print(f\"✓ Extra dataset found\")\n",
    "#     else:\n",
    "#         print(f\"✗ Extra dataset not found at {config.extra_dataset_path}\")\n",
    "    \n",
    "#     for name, path in datasets.items():\n",
    "#         if not pathlib.Path(path).exists():\n",
    "#             print(f\"✗ Dataset not found: {path}\")\n",
    "#             continue\n",
    "        \n",
    "#         videos = list(pathlib.Path(path).glob(\"**/*.mp4\"))\n",
    "#         print(f\"{name}: {len(videos)} videos discovered\")\n",
    "        \n",
    "#         # Group by class\n",
    "#         by_class = {}\n",
    "#         for video in videos:\n",
    "#             class_name = video.parent.name\n",
    "#             if class_name not in by_class:\n",
    "#                 by_class[class_name] = []\n",
    "#             by_class[class_name].append(video)\n",
    "        \n",
    "#         print(f\"  Classes in {name}: {list(by_class.keys())}\")\n",
    "        \n",
    "#         for class_name, class_videos in by_class.items():\n",
    "#             # Testing mode (if enabled)\n",
    "#             if config.testing_mode and config.samples_per_class:\n",
    "#                 random.shuffle(class_videos)\n",
    "#                 class_videos = class_videos[:config.samples_per_class]\n",
    "#                 print(f\"  {class_name}: {len(class_videos)} videos (limited for testing)\")\n",
    "            \n",
    "#             for video in class_videos:\n",
    "#                 # Split assignment\n",
    "#                 if name == 'extra':\n",
    "#                     split = 'train' if random.random() < config.extra_train_ratio else 'val'\n",
    "#                 else:\n",
    "#                     split = name\n",
    "                \n",
    "#                 records.append({\n",
    "#                     'video_path': str(video),\n",
    "#                     'video_name': video.name,\n",
    "#                     'class_name': class_name,\n",
    "#                     'original_dir': name,\n",
    "#                     'split': split,\n",
    "#                     'ocr_text': \"\",\n",
    "#                     'audio_text': \"\",\n",
    "#                     'combined_text': \"\",\n",
    "#                     'processing_status': \"pending\",\n",
    "#                     'error_message': \"\"\n",
    "#                 })\n",
    "    \n",
    "#     print(f\"\\n{'='*50}\")\n",
    "#     print(f\"TOTAL VIDEOS DISCOVERED: {len(records)}\")\n",
    "#     print(f\"{'='*50}\")\n",
    "    \n",
    "#     # Show distribution\n",
    "#     for dataset in ['train', 'val', 'test', 'extra']:\n",
    "#         dataset_count = len([r for r in records if r['original_dir'] == dataset])\n",
    "#         if dataset_count > 0:\n",
    "#             print(f\"  {dataset}: {dataset_count} videos\")\n",
    "    \n",
    "#     split_counts = {}\n",
    "#     for record in records:\n",
    "#         split = record['split']\n",
    "#         split_counts[split] = split_counts.get(split, 0) + 1\n",
    "    \n",
    "#     print(f\"\\nSplit distribution:\")\n",
    "#     for split, count in split_counts.items():\n",
    "#         print(f\"  {split}: {count} videos\")\n",
    "    \n",
    "#     return records\n",
    "\n",
    "# def run_complete_extraction():\n",
    "#     \"\"\"Complete extraction ensuring all 4723 samples are processed\"\"\"\n",
    "#     print(\"=\" * 60)\n",
    "#     print(\"COMPLETE VI+EN EXTRACTION - ALL 4723 SAMPLES GUARANTEED\")\n",
    "#     print(\"=\" * 60)\n",
    "    \n",
    "#     # Discover all videos\n",
    "#     all_videos = discover_all_videos()\n",
    "    \n",
    "#     if not all_videos:\n",
    "#         print(\"ERROR: No videos found!\")\n",
    "#         return None\n",
    "    \n",
    "#     total_videos = len(all_videos)\n",
    "#     print(f\"\\nStarting processing of {total_videos} videos...\")\n",
    "#     print(f\"Target: Process ALL {total_videos} samples regardless of success/failure\")\n",
    "#     print(f\"Features: Spam filter + Hallucination filter + Vi/En detection\")\n",
    "    \n",
    "#     # Initialize processor\n",
    "#     processor = CompleteProcessor()\n",
    "#     processor.total_discovered = total_videos\n",
    "    \n",
    "#     start_time = time.time()\n",
    "#     successful_processing = 0\n",
    "    \n",
    "#     try:\n",
    "#         for i, video in enumerate(all_videos):\n",
    "#             # Process video (always returns a record)\n",
    "#             processed_video = processor.process_video_safe(video)\n",
    "            \n",
    "#             # Update the original record\n",
    "#             all_videos[i] = processed_video\n",
    "            \n",
    "#             if processed_video['combined_text']:\n",
    "#                 successful_processing += 1\n",
    "            \n",
    "#             # Progress updates\n",
    "#             if (i + 1) % config.progress_interval == 0 or (i + 1) == total_videos:\n",
    "#                 elapsed = time.time() - start_time\n",
    "#                 speed = (i + 1) / (elapsed / 60) if elapsed > 0 else 0\n",
    "#                 eta = (total_videos - i - 1) / max(speed, 1) if speed > 0 else 0\n",
    "                \n",
    "#                 stats = processor.get_comprehensive_stats()\n",
    "                \n",
    "#                 print(f\"\\n{'='*60}\")\n",
    "#                 print(f\"Progress: {i + 1}/{total_videos} ({(i + 1)/total_videos*100:.1f}%)\")\n",
    "#                 print(f\"Speed: {speed:.1f} videos/min\")\n",
    "#                 if eta > 0:\n",
    "#                     print(f\"ETA: {eta:.1f} minutes\")\n",
    "#                 print(f\"\")\n",
    "#                 print(f\"Results so far:\")\n",
    "#                 print(f\"  Processed: {stats['total_processed']}/{total_videos}\")\n",
    "#                 print(f\"  Completed: {stats['total_completed']}/{total_videos}\")\n",
    "#                 print(f\"  Audio success: {stats['audio_success']} ({stats['audio_success']/(i+1)*100:.1f}%)\")\n",
    "#                 print(f\"  OCR success: {stats['ocr_success']} ({stats['ocr_success']/(i+1)*100:.1f}%)\")\n",
    "#                 print(f\"  Combined success: {stats['combined_success']} ({stats['combined_success']/(i+1)*100:.1f}%)\")\n",
    "#                 print(f\"  Processing errors: {stats['processing_errors']}\")\n",
    "#                 print(f\"  Languages - Vi: {stats['vietnamese_count']}, En: {stats['english_count']}\")\n",
    "#                 print(f\"  Avg time - Audio: {stats['avg_audio_time']:.1f}s, OCR: {stats['avg_ocr_time']:.1f}s\")\n",
    "#                 print(f\"{'='*60}\")\n",
    "            \n",
    "#             # Memory cleanup\n",
    "#             if (i + 1) % config.cleanup_interval == 0:\n",
    "#                 processor.cleanup()\n",
    "    \n",
    "#     except KeyboardInterrupt:\n",
    "#         print(f\"\\n⚠️  INTERRUPTED after {processor.total_processed} videos\")\n",
    "#         print(f\"Saving partial results...\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\n❌ FATAL ERROR after {processor.total_processed} videos: {e}\")\n",
    "#         print(f\"Saving partial results...\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "    \n",
    "#     finally:\n",
    "#         processor.cleanup()\n",
    "    \n",
    "#     # Create final DataFrame - GUARANTEED to have all discovered videos\n",
    "#     print(f\"\\nCreating final DataFrame...\")\n",
    "#     df = pd.DataFrame(all_videos)\n",
    "    \n",
    "#     # Verify we have all samples\n",
    "#     final_count = len(df)\n",
    "#     print(f\"DataFrame created with {final_count} samples\")\n",
    "    \n",
    "#     if final_count != total_videos:\n",
    "#         print(f\"⚠️  WARNING: Expected {total_videos} samples, got {final_count}\")\n",
    "#     else:\n",
    "#         print(f\"✅ SUCCESS: All {total_videos} samples preserved in DataFrame\")\n",
    "    \n",
    "#     # Final statistics\n",
    "#     processing_time = time.time() - start_time\n",
    "#     stats = processor.get_comprehensive_stats()\n",
    "    \n",
    "#     # Count actual results\n",
    "#     audio_total = (df['audio_text'].str.strip() != '').sum()\n",
    "#     ocr_total = (df['ocr_text'].str.strip() != '').sum()\n",
    "#     combined_total = (df['combined_text'].str.strip() != '').sum()\n",
    "#     error_total = (df['processing_status'] == 'error').sum()\n",
    "    \n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(\"FINAL COMPLETE RESULTS\")\n",
    "#     print(f\"{'='*60}\")\n",
    "#     print(f\"Total samples in dataset: {final_count}\")\n",
    "#     print(f\"Successfully processed: {stats['total_completed']}\")\n",
    "#     print(f\"Audio extraction success: {audio_total} ({audio_total/final_count*100:.1f}%)\")\n",
    "#     print(f\"OCR extraction success: {ocr_total} ({ocr_total/final_count*100:.1f}%)\")\n",
    "#     print(f\"Combined text success: {combined_total} ({combined_total/final_count*100:.1f}%)\")\n",
    "#     print(f\"Processing errors: {error_total} ({error_total/final_count*100:.1f}%)\")\n",
    "#     print(f\"Empty results (no text): {final_count - combined_total} ({(final_count - combined_total)/final_count*100:.1f}%)\")\n",
    "#     print(f\"\")\n",
    "#     print(f\"Language distribution:\")\n",
    "#     print(f\"  Vietnamese: {stats['vietnamese_count']}\")\n",
    "#     print(f\"  English: {stats['english_count']}\")\n",
    "#     print(f\"\")\n",
    "#     print(f\"Processing time: {processing_time/60:.1f} minutes\")\n",
    "#     print(f\"Average speed: {final_count/(processing_time/60):.1f} videos/minute\")\n",
    "#     print(f\"Average per video: {processing_time/final_count:.1f} seconds\")\n",
    "    \n",
    "#     # Dataset breakdown - COMPLETE\n",
    "#     print(f\"\\nComplete dataset breakdown:\")\n",
    "#     for dataset in ['train', 'val', 'test', 'extra']:\n",
    "#         dataset_df = df[df['original_dir'] == dataset]\n",
    "#         if len(dataset_df) > 0:\n",
    "#             with_text = (dataset_df['combined_text'].str.strip() != '').sum()\n",
    "#             print(f\"  {dataset}: {len(dataset_df)} videos, {with_text} with text ({with_text/len(dataset_df)*100:.1f}%)\")\n",
    "    \n",
    "#     print(f\"\\nSplit breakdown:\")\n",
    "#     for split in ['train', 'val', 'test']:\n",
    "#         split_df = df[df['split'] == split]\n",
    "#         if len(split_df) > 0:\n",
    "#             with_text = (split_df['combined_text'].str.strip() != '').sum()\n",
    "#             print(f\"  {split}: {len(split_df)} videos, {with_text} with text ({with_text/len(split_df)*100:.1f}%)\")\n",
    "    \n",
    "#     # Save complete results\n",
    "#     print(f\"\\nSaving complete dataset...\")\n",
    "#     df.to_csv(config.output_file, index=False)\n",
    "#     print(f\"✅ Complete dataset saved: {config.output_file}\")\n",
    "#     print(f\"   Contains all {len(df)} samples with columns:\")\n",
    "#     print(f\"   {list(df.columns)}\")\n",
    "    \n",
    "#     # Show sample successful results\n",
    "#     successful = df[df['combined_text'].str.strip() != ''].head(3)\n",
    "#     if len(successful) > 0:\n",
    "#         print(f\"\\nSample successful extractions:\")\n",
    "#         for _, row in successful.iterrows():\n",
    "#             print(f\"  📁 {row['video_name']} ({row['class_name']})\")\n",
    "#             if row['audio_text']:\n",
    "#                 print(f\"     🎵 Audio: {row['audio_text'][:80]}{'...' if len(row['audio_text']) > 80 else ''}\")\n",
    "#             if row['ocr_text']:\n",
    "#                 print(f\"     🖼️  OCR: {row['ocr_text'][:80]}{'...' if len(row['ocr_text']) > 80 else ''}\")\n",
    "    \n",
    "#     # Show sample empty results\n",
    "#     empty = df[df['combined_text'].str.strip() == ''].head(2)\n",
    "#     if len(empty) > 0:\n",
    "#         print(f\"\\nSample empty results (still preserved):\")\n",
    "#         for _, row in empty.iterrows():\n",
    "#             print(f\"  📁 {row['video_name']} ({row['class_name']}) - Status: {row['processing_status']}\")\n",
    "#             if row['error_message']:\n",
    "#                 print(f\"     ⚠️  Error: {row['error_message'][:60]}...\")\n",
    "    \n",
    "#     # Show filtering effectiveness\n",
    "#     if stats['hallucination_filtered'] > 0:\n",
    "#         print(f\"\\n🛡️  Hallucination filter effectiveness:\")\n",
    "#         print(f\"   Filtered out: {stats['hallucination_filtered']} potential hallucinations\")\n",
    "#         print(f\"   Quality improvement: Reduced false positive extractions\")\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # Execute complete extraction\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         # CUDA optimizations\n",
    "#         if torch.cuda.is_available():\n",
    "#             torch.backends.cudnn.benchmark = True\n",
    "#             torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        \n",
    "#         # Seeds for reproducibility\n",
    "#         random.seed(42)\n",
    "#         np.random.seed(42)\n",
    "#         torch.manual_seed(42)\n",
    "        \n",
    "#         # Run complete extraction\n",
    "#         results = run_complete_extraction()\n",
    "        \n",
    "#         if results is not None:\n",
    "#             print(f\"\\n🎉 COMPLETE SUCCESS!\")\n",
    "#             print(f\"📊 All 4723 samples processed and saved\")\n",
    "#             print(f\"🇻🇳🇺🇸 Vietnamese + English extraction completed\")\n",
    "#             print(f\"🛡️  Spam + Hallucination filtering applied\")\n",
    "#             print(f\"💾 Complete dataset ready for use\")\n",
    "#             print(f\"\")\n",
    "#             print(f\"Final verification:\")\n",
    "#             print(f\"  Expected samples: 4723\")\n",
    "#             print(f\"  Actual samples: {len(results)}\")\n",
    "#             print(f\"  Match: {'✅ YES' if len(results) == 4723 else '❌ NO'}\")\n",
    "#         else:\n",
    "#             print(f\"\\n❌ FAILED - No results generated\")\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nFATAL ERROR: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "    \n",
    "#     finally:\n",
    "#         gc.collect()\n",
    "#         if torch.cuda.is_available():\n",
    "#             torch.cuda.empty_cache()\n",
    "#         print(\"\\nProcessing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c89f92",
   "metadata": {
    "papermill": {
     "duration": 0.003818,
     "end_time": "2025-07-01T16:14:36.063904",
     "exception": false,
     "start_time": "2025-07-01T16:14:36.060086",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phase 2: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "408c92b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T16:14:36.073088Z",
     "iopub.status.busy": "2025-07-01T16:14:36.072506Z",
     "iopub.status.idle": "2025-07-01T16:16:20.216680Z",
     "shell.execute_reply": "2025-07-01T16:16:20.215785Z"
    },
    "papermill": {
     "duration": 104.151072,
     "end_time": "2025-07-01T16:16:20.218916",
     "exception": false,
     "start_time": "2025-07-01T16:14:36.067844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\r\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\r\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.7/132.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install required packages first\n",
    "!pip install -q transformers>=4.52.0 accelerate>=1.0.0 datasets>=3.0.0\n",
    "!pip install -q av librosa soundfile resampy\n",
    "!pip install -q scikit-learn matplotlib seaborn\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install -q pytorchvideo\n",
    "!pip install -q evaluate\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "781e8a39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T16:16:20.233369Z",
     "iopub.status.busy": "2025-07-01T16:16:20.233101Z",
     "iopub.status.idle": "2025-07-01T23:42:04.915323Z",
     "shell.execute_reply": "2025-07-01T23:42:04.914406Z"
    },
    "papermill": {
     "duration": 26744.693179,
     "end_time": "2025-07-01T23:42:04.917892",
     "exception": false,
     "start_time": "2025-07-01T16:16:20.224713",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 16:16:37.580290: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751386597.772844      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751386597.828632      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SMART MULTIMODAL CLASSIFIER - ADAPTIVE VALIDATION (FIXED) ===\n",
      "🚀 Starting smart adaptive training (FIXED VERSION)...\n",
      "📂 Loading dataset...\n",
      "Dataset loaded: 4723 samples\n",
      "Classes distribution:\n",
      "class_name\n",
      "Safe               1248\n",
      "Harmful Content    1193\n",
      "Adult Content      1141\n",
      "Suicide            1141\n",
      "Name: count, dtype: int64\n",
      "📊 Classes (4): ['Adult Content', 'Harmful Content', 'Safe', 'Suicide']\n",
      "\n",
      "🔍 Sample video paths:\n",
      "  /kaggle/input/tikharm-dataset/Dataset/train/Adult Content/jaypark_foryou_7339639860354911493.mp4 - Exists: True\n",
      "  /kaggle/input/tikharm-dataset/Dataset/train/Adult Content/1life.anthonyblane_7368846422709521671.mp4 - Exists: True\n",
      "  /kaggle/input/tikharm-dataset/Dataset/train/Adult Content/loungebarvip_7252268249243454725.mp4 - Exists: True\n",
      "  /kaggle/input/tikharm-dataset/Dataset/train/Adult Content/phangphong111_7287936860796620039.mp4 - Exists: True\n",
      "  /kaggle/input/tikharm-dataset/Dataset/train/Adult Content/girl_xinh.sexy_7298717939052711170.mp4 - Exists: True\n",
      "🔍 Initializing smart video validator...\n",
      "⚠️ FFprobe not available - will use OpenCV-only validation\n",
      "📋 Validator initialized:\n",
      "  FFprobe available: False\n",
      "  Strict mode: False\n",
      "🧹 Creating smart validated splits...\n",
      "Original splits - Train: 2762, Val: 396, Test: 790\n",
      "Validating training set...\n",
      "🧪 Testing validation on 100 samples...\n",
      "📊 Sample validation results:\n",
      "  Tested: 100 files\n",
      "  Valid: 100\n",
      "  Success rate: 100.0%\n",
      "🔍 Performing normal video validation...\n",
      "  Progress: 500/2762 files validated...\n",
      "  Progress: 1000/2762 files validated...\n",
      "  Progress: 1500/2762 files validated...\n",
      "  Progress: 2000/2762 files validated...\n",
      "  Progress: 2500/2762 files validated...\n",
      "Validating validation set...\n",
      "🧪 Testing validation on 50 samples...\n",
      "📊 Sample validation results:\n",
      "  Tested: 50 files\n",
      "  Valid: 50\n",
      "  Success rate: 100.0%\n",
      "🔍 Performing normal video validation...\n",
      "Validating test set...\n",
      "🧪 Testing validation on 50 samples...\n",
      "📊 Sample validation results:\n",
      "  Tested: 50 files\n",
      "  Valid: 50\n",
      "  Success rate: 100.0%\n",
      "🔍 Performing normal video validation...\n",
      "  Progress: 500/790 files validated...\n",
      "\n",
      "📊 Final validation statistics:\n",
      "  Total files: 3948\n",
      "  ✅ Valid files: 3948\n",
      "  ❌ Invalid files: 0\n",
      "  📁 Not found: 0\n",
      "  📏 Too small: 0\n",
      "  📹 OpenCV success: 3948\n",
      "  📹 OpenCV failed: 0\n",
      "  📈 Overall success rate: 100.0%\n",
      "\n",
      "✨ Final splits:\n",
      "  🏋️ Train: 2762 (was 2762)\n",
      "  ✅ Val: 396 (was 396)\n",
      "  🧪 Test: 790 (was 790)\n",
      "\n",
      "🔍 VERIFICATION - Actual dataframe sizes:\n",
      "  Train DF: 2762 rows\n",
      "  Val DF: 396 rows\n",
      "  Test DF: 790 rows\n",
      "🧠 Initializing models...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d5c7f3c378450eb27ef595163aa00b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "663cd064b5c14c97965e748726283143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045ca771ec3844cab6df46f09734f96f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe483f860ab48019aca5f927bc82970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💻 Using device: cuda\n",
      "🔤 Initializing text model: distilbert-base-multilingual-cased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef893dd577784626b068e6896b99e440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/542M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎬 Initializing video model: facebook/timesformer-base-finetuned-k400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abf43c660b034d85967121d73cbba718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0834c16064ed4bdc8706be3bc3198bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/486M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📐 Model dimensions - Text: 768, Video: 768\n",
      "🧠 Model: 256,555,144 total, 256,555,144 trainable\n",
      "📊 Creating smart datasets with tracking...\n",
      "🛡️ TRAIN dataset: 2762 samples (verified)\n",
      "🛡️ VAL dataset: 396 samples (verified)\n",
      "🛡️ TEST dataset: 790 samples (verified)\n",
      "\n",
      "🎯 FINAL DATASET VERIFICATION:\n",
      "  Train dataset: 2762 samples\n",
      "  Val dataset: 396 samples\n",
      "  Test dataset: 790 samples\n",
      "🧪 Testing sample loading...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "540637dcf2654bb79936dfbe3e68c983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/486M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Video shape: torch.Size([8, 3, 224, 224])\n",
      "  ✅ Text shape: torch.Size([64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Output shape: torch.Size([1, 4])\n",
      "\n",
      "================================================================================\n",
      "🚀 STARTING SMART ADAPTIVE TRAINING (FIXED)!\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1038' max='1038' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1038/1038 7:04:28, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.377300</td>\n",
       "      <td>0.832425</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.775290</td>\n",
       "      <td>0.777872</td>\n",
       "      <td>0.786832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.564100</td>\n",
       "      <td>0.733158</td>\n",
       "      <td>0.800505</td>\n",
       "      <td>0.800105</td>\n",
       "      <td>0.800193</td>\n",
       "      <td>0.817634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.474300</td>\n",
       "      <td>0.675323</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.846743</td>\n",
       "      <td>0.848326</td>\n",
       "      <td>0.855256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.330100</td>\n",
       "      <td>0.693130</td>\n",
       "      <td>0.838384</td>\n",
       "      <td>0.837591</td>\n",
       "      <td>0.838352</td>\n",
       "      <td>0.850635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.258900</td>\n",
       "      <td>0.706666</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.846578</td>\n",
       "      <td>0.848482</td>\n",
       "      <td>0.860118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.199400</td>\n",
       "      <td>0.738048</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833003</td>\n",
       "      <td>0.833810</td>\n",
       "      <td>0.843735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.195300</td>\n",
       "      <td>0.730099</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.842149</td>\n",
       "      <td>0.841183</td>\n",
       "      <td>0.854065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.042700</td>\n",
       "      <td>0.705074</td>\n",
       "      <td>0.853535</td>\n",
       "      <td>0.854745</td>\n",
       "      <td>0.853400</td>\n",
       "      <td>0.858301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.032700</td>\n",
       "      <td>0.705230</td>\n",
       "      <td>0.845960</td>\n",
       "      <td>0.846958</td>\n",
       "      <td>0.845568</td>\n",
       "      <td>0.859192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.986300</td>\n",
       "      <td>0.682787</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.863010</td>\n",
       "      <td>0.863656</td>\n",
       "      <td>0.870290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.909700</td>\n",
       "      <td>0.733203</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.847954</td>\n",
       "      <td>0.848659</td>\n",
       "      <td>0.866091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.852100</td>\n",
       "      <td>0.668345</td>\n",
       "      <td>0.871212</td>\n",
       "      <td>0.871298</td>\n",
       "      <td>0.871335</td>\n",
       "      <td>0.875776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.826600</td>\n",
       "      <td>0.652038</td>\n",
       "      <td>0.883838</td>\n",
       "      <td>0.883625</td>\n",
       "      <td>0.883757</td>\n",
       "      <td>0.885502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.811700</td>\n",
       "      <td>0.643728</td>\n",
       "      <td>0.883838</td>\n",
       "      <td>0.882873</td>\n",
       "      <td>0.883655</td>\n",
       "      <td>0.886841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.746800</td>\n",
       "      <td>0.660051</td>\n",
       "      <td>0.881313</td>\n",
       "      <td>0.880880</td>\n",
       "      <td>0.881232</td>\n",
       "      <td>0.884506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.792700</td>\n",
       "      <td>0.660446</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>0.877748</td>\n",
       "      <td>0.878630</td>\n",
       "      <td>0.879530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.755900</td>\n",
       "      <td>0.666834</td>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.885634</td>\n",
       "      <td>0.886180</td>\n",
       "      <td>0.886200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.746500</td>\n",
       "      <td>0.669166</td>\n",
       "      <td>0.883838</td>\n",
       "      <td>0.883351</td>\n",
       "      <td>0.883783</td>\n",
       "      <td>0.884684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.746300</td>\n",
       "      <td>0.672315</td>\n",
       "      <td>0.883838</td>\n",
       "      <td>0.882827</td>\n",
       "      <td>0.883681</td>\n",
       "      <td>0.889873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.748600</td>\n",
       "      <td>0.663472</td>\n",
       "      <td>0.883838</td>\n",
       "      <td>0.882849</td>\n",
       "      <td>0.883629</td>\n",
       "      <td>0.885780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎉 TRAINING COMPLETED in 25493.7s!\n",
      "💾 Model saved!\n",
      "📊 Running evaluation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 05:56]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 VALIDATION RESULTS:\n",
      "  Accuracy: 0.8864 (88.64%)\n",
      "  F1-Score: 0.8856\n",
      "  Recall: 0.8862\n",
      "  Precision: 0.8862\n",
      "🧪 Running FIXED test evaluation...\n",
      "🔍 Processing test data in 99 batches...\n",
      "  Processed batch 10/99\n",
      "  Processed batch 20/99\n",
      "  Processed batch 30/99\n",
      "  Processed batch 40/99\n",
      "  Processed batch 50/99\n",
      "  Processed batch 60/99\n",
      "  Processed batch 70/99\n",
      "  Processed batch 80/99\n",
      "  Processed batch 90/99\n",
      "\n",
      "🎯 FINAL TEST DATA VERIFICATION:\n",
      "  Expected test samples: 790\n",
      "  Actual predictions: 790\n",
      "  Actual labels: 790\n",
      "  Data consistency: ✅ CONSISTENT\n",
      "\n",
      "🏆 FINAL TEST RESULTS (VERIFIED):\n",
      "  Test samples: 790\n",
      "  Accuracy: 0.8937 (89.37%)\n",
      "  F1-Score: 0.8936\n",
      "  Recall: 0.8936\n",
      "  Precision: 0.8945\n",
      "\n",
      "📋 CLASSIFICATION REPORT (VERIFIED):\n",
      "------------------------------------------------------------\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "  Adult Content     0.9149    0.8821    0.8982       195\n",
      "Harmful Content     0.8750    0.8485    0.8615       198\n",
      "           Safe     0.9289    0.9150    0.9219       200\n",
      "        Suicide     0.8592    0.9289    0.8927       197\n",
      "\n",
      "       accuracy                         0.8937       790\n",
      "      macro avg     0.8945    0.8936    0.8936       790\n",
      "   weighted avg     0.8946    0.8937    0.8936       790\n",
      "\n",
      "💾 Results saved!\n",
      "\n",
      "📈 Final statistics:\n",
      "📊 TRAIN stats:\n",
      "  DataFrame length: 2762\n",
      "  Actual length: 2762\n",
      "📹 Video loading: 16573/16573 successful (100.0%)\n",
      "📊 VAL stats:\n",
      "  DataFrame length: 396\n",
      "  Actual length: 396\n",
      "📹 Video loading: 8316/8316 successful (100.0%)\n",
      "📊 TEST stats:\n",
      "  DataFrame length: 790\n",
      "  Actual length: 790\n",
      "📹 Video loading: 790/790 successful (100.0%)\n",
      "\n",
      "================================================================================\n",
      "🎊 SMART TRAINING COMPLETED SUCCESSFULLY (FIXED)!\n",
      "✅ DATA COUNTING ISSUES RESOLVED!\n",
      "📊 ALL METRICS CALCULATED CORRECTLY!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "import warnings\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, recall_score, precision_score\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import subprocess\n",
    "import sys\n",
    "from contextlib import contextmanager, redirect_stderr\n",
    "import io\n",
    "\n",
    "# Transformers components\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoImageProcessor,\n",
    "    TimesformerForVideoClassification, Trainer, TrainingArguments, \n",
    "    EvalPrediction, PreTrainedModel, PretrainedConfig\n",
    ")\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import evaluate\n",
    "\n",
    "# =============================================================================\n",
    "# COMPLETE ERROR SUPPRESSION AND ENVIRONMENT SETUP\n",
    "# =============================================================================\n",
    "\n",
    "def setup_ultimate_silent_environment():\n",
    "    \"\"\"Setup the most comprehensive silent environment\"\"\"\n",
    "    \n",
    "    # Environment variables for complete silence\n",
    "    silent_vars = {\n",
    "        'OPENCV_LOG_LEVEL': 'SILENT',\n",
    "        'OPENCV_VIDEOIO_DEBUG': '0',\n",
    "        'OPENCV_FFMPEG_DEBUG': '0',\n",
    "        'OPENCV_VIDEOWRITER_DEBUG': '0',\n",
    "        'OPENCV_VIDEOCAPTURE_DEBUG': '0',\n",
    "        'OPENCV_FFMPEG_LOGLEVEL': '-8',\n",
    "        'OPENCV_AVFOUNDATION_SKIP_AUTH': '1',\n",
    "        'FFREPORT': 'file=/dev/null:level=quiet',\n",
    "        'WANDB_DISABLED': 'true',\n",
    "        'TOKENIZERS_PARALLELISM': 'false',\n",
    "        'PYTHONWARNINGS': 'ignore',\n",
    "        'CUDA_LAUNCH_BLOCKING': '0'\n",
    "    }\n",
    "    \n",
    "    for key, value in silent_vars.items():\n",
    "        os.environ[key] = value\n",
    "    \n",
    "    # Configure OpenCV for complete silence\n",
    "    try:\n",
    "        cv2.setLogLevel(0)\n",
    "        cv2.setUseOptimized(True)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Suppress all warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    warnings.simplefilter('ignore')\n",
    "\n",
    "@contextmanager\n",
    "def ultimate_stderr_suppression():\n",
    "    \"\"\"Most advanced stderr suppression for C libraries\"\"\"\n",
    "    old_stderr = None\n",
    "    try:\n",
    "        if hasattr(os, 'devnull'):\n",
    "            old_stderr = sys.stderr\n",
    "            sys.stderr = open(os.devnull, 'w')\n",
    "            yield\n",
    "        else:\n",
    "            old_stderr = sys.stderr\n",
    "            sys.stderr = io.StringIO()\n",
    "            yield\n",
    "    except:\n",
    "        yield\n",
    "    finally:\n",
    "        if old_stderr is not None:\n",
    "            try:\n",
    "                if hasattr(sys.stderr, 'close'):\n",
    "                    sys.stderr.close()\n",
    "            except:\n",
    "                pass\n",
    "            sys.stderr = old_stderr\n",
    "\n",
    "# Setup environment immediately\n",
    "setup_ultimate_silent_environment()\n",
    "print(\"=== SMART MULTIMODAL CLASSIFIER - ADAPTIVE VALIDATION (FIXED) ===\")\n",
    "\n",
    "def cleanup_memory():\n",
    "    \"\"\"Enhanced memory cleanup\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "cleanup_memory()\n",
    "\n",
    "# =============================================================================\n",
    "# 1. SMART VIDEO VALIDATION SYSTEM (SAME AS BEFORE)\n",
    "# =============================================================================\n",
    "\n",
    "class SmartVideoValidator:\n",
    "    \"\"\"Smart video validator with adaptive fallback strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, strict_mode=False):\n",
    "        self.strict_mode = strict_mode\n",
    "        self.validation_cache = {}\n",
    "        self.ffprobe_available = None\n",
    "        self.stats = {\n",
    "            'total': 0, 'valid': 0, 'invalid': 0,\n",
    "            'not_found': 0, 'too_small': 0, \n",
    "            'ffprobe_failed': 0, 'opencv_failed': 0,\n",
    "            'opencv_success': 0, 'ffprobe_success': 0\n",
    "        }\n",
    "        \n",
    "        # Check FFprobe availability\n",
    "        self._check_ffprobe_availability()\n",
    "        \n",
    "        print(f\"📋 Validator initialized:\")\n",
    "        print(f\"  FFprobe available: {self.ffprobe_available}\")\n",
    "        print(f\"  Strict mode: {self.strict_mode}\")\n",
    "    \n",
    "    def _check_ffprobe_availability(self):\n",
    "        \"\"\"Check if FFprobe is available on the system\"\"\"\n",
    "        try:\n",
    "            with ultimate_stderr_suppression():\n",
    "                result = subprocess.run(['ffprobe', '-version'], \n",
    "                                      capture_output=True, \n",
    "                                      timeout=5)\n",
    "            self.ffprobe_available = (result.returncode == 0)\n",
    "        except:\n",
    "            self.ffprobe_available = False\n",
    "        \n",
    "        if not self.ffprobe_available:\n",
    "            print(\"⚠️ FFprobe not available - will use OpenCV-only validation\")\n",
    "    \n",
    "    def is_video_valid(self, video_path: str) -> bool:\n",
    "        \"\"\"Smart video validation with adaptive strategies\"\"\"\n",
    "        \n",
    "        if video_path in self.validation_cache:\n",
    "            return self.validation_cache[video_path]\n",
    "        \n",
    "        self.stats['total'] += 1\n",
    "        is_valid = False\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Basic checks\n",
    "            if not video_path or not os.path.exists(video_path):\n",
    "                self.stats['not_found'] += 1\n",
    "                self.validation_cache[video_path] = False\n",
    "                return False\n",
    "            \n",
    "            # Step 2: File size check (more lenient)\n",
    "            try:\n",
    "                file_size = os.path.getsize(video_path)\n",
    "                if file_size < 1000:  # Less than 1KB\n",
    "                    self.stats['too_small'] += 1\n",
    "                    self.validation_cache[video_path] = False\n",
    "                    return False\n",
    "            except:\n",
    "                self.stats['too_small'] += 1\n",
    "                self.validation_cache[video_path] = False\n",
    "                return False\n",
    "            \n",
    "            # Step 3: Try FFprobe if available, otherwise use OpenCV\n",
    "            if self.ffprobe_available:\n",
    "                if self._validate_with_ffprobe(video_path):\n",
    "                    is_valid = True\n",
    "                    self.stats['ffprobe_success'] += 1\n",
    "                else:\n",
    "                    self.stats['ffprobe_failed'] += 1\n",
    "                    # If FFprobe fails, try OpenCV as fallback\n",
    "                    if self._validate_with_opencv(video_path):\n",
    "                        is_valid = True\n",
    "                        self.stats['opencv_success'] += 1\n",
    "                    else:\n",
    "                        self.stats['opencv_failed'] += 1\n",
    "            else:\n",
    "                # OpenCV-only validation\n",
    "                if self._validate_with_opencv(video_path):\n",
    "                    is_valid = True\n",
    "                    self.stats['opencv_success'] += 1\n",
    "                else:\n",
    "                    self.stats['opencv_failed'] += 1\n",
    "                \n",
    "        except:\n",
    "            self.stats['invalid'] += 1\n",
    "        \n",
    "        if not is_valid:\n",
    "            self.stats['invalid'] += 1\n",
    "        else:\n",
    "            self.stats['valid'] += 1\n",
    "        \n",
    "        self.validation_cache[video_path] = is_valid\n",
    "        return is_valid\n",
    "    \n",
    "    def _validate_with_ffprobe(self, video_path: str) -> bool:\n",
    "        \"\"\"FFprobe validation with lenient criteria\"\"\"\n",
    "        try:\n",
    "            cmd = [\n",
    "                'ffprobe', '-v', 'quiet', '-select_streams', 'v:0',\n",
    "                '-show_entries', 'stream=width,height,nb_frames',\n",
    "                '-of', 'csv=p=0', video_path\n",
    "            ]\n",
    "            \n",
    "            with ultimate_stderr_suppression():\n",
    "                result = subprocess.run(cmd, capture_output=True, text=True, timeout=10)\n",
    "            \n",
    "            if result.returncode != 0:\n",
    "                return False\n",
    "            \n",
    "            # Parse output\n",
    "            output = result.stdout.strip()\n",
    "            if not output:\n",
    "                return False\n",
    "            \n",
    "            try:\n",
    "                parts = output.split(',')\n",
    "                if len(parts) >= 2:\n",
    "                    width = int(parts[0]) if parts[0] else 0\n",
    "                    height = int(parts[1]) if parts[1] else 0\n",
    "                    \n",
    "                    # Very lenient criteria\n",
    "                    if width > 0 and height > 0:\n",
    "                        return True\n",
    "            except (ValueError, IndexError):\n",
    "                pass\n",
    "            \n",
    "            return False\n",
    "            \n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def _validate_with_opencv(self, video_path: str) -> bool:\n",
    "        \"\"\"OpenCV validation with lenient criteria\"\"\"\n",
    "        cap = None\n",
    "        try:\n",
    "            with ultimate_stderr_suppression():\n",
    "                # Try different backends\n",
    "                for backend in [cv2.CAP_FFMPEG, cv2.CAP_ANY]:\n",
    "                    try:\n",
    "                        cap = cv2.VideoCapture(video_path, backend)\n",
    "                        if cap and cap.isOpened():\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if not cap or not cap.isOpened():\n",
    "                    return False\n",
    "                \n",
    "                # Get basic properties\n",
    "                frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                \n",
    "                # Very lenient criteria - just check if we can get basic properties\n",
    "                if width > 0 and height > 0:\n",
    "                    # Try to read at least one frame\n",
    "                    ret, frame = cap.read()\n",
    "                    if ret and frame is not None and frame.size > 0:\n",
    "                        return True\n",
    "                \n",
    "                return False\n",
    "                \n",
    "        except:\n",
    "            return False\n",
    "        finally:\n",
    "            if cap:\n",
    "                try:\n",
    "                    cap.release()\n",
    "                except:\n",
    "                    pass\n",
    "    \n",
    "    def validate_dataset_sample(self, df: pd.DataFrame, sample_size: int = 100) -> pd.DataFrame:\n",
    "        \"\"\"Validate a sample first to check validation success rate\"\"\"\n",
    "        print(f\"🧪 Testing validation on {sample_size} samples...\")\n",
    "        \n",
    "        sample_df = df.head(sample_size) if len(df) > sample_size else df\n",
    "        valid_count = 0\n",
    "        \n",
    "        for idx, row in sample_df.iterrows():\n",
    "            video_path = row['video_path']\n",
    "            if self.is_video_valid(video_path):\n",
    "                valid_count += 1\n",
    "        \n",
    "        success_rate = (valid_count / len(sample_df)) * 100\n",
    "        print(f\"📊 Sample validation results:\")\n",
    "        print(f\"  Tested: {len(sample_df)} files\")\n",
    "        print(f\"  Valid: {valid_count}\")\n",
    "        print(f\"  Success rate: {success_rate:.1f}%\")\n",
    "        \n",
    "        if success_rate < 10:\n",
    "            print(\"⚠️ Very low validation success rate!\")\n",
    "            print(\"🔄 Switching to lenient OpenCV-only mode...\")\n",
    "            return self._validate_dataset_lenient(df)\n",
    "        else:\n",
    "            return self._validate_dataset_normal(df)\n",
    "    \n",
    "    def _validate_dataset_normal(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Normal validation process\"\"\"\n",
    "        print(\"🔍 Performing normal video validation...\")\n",
    "        \n",
    "        valid_indices = []\n",
    "        total = len(df)\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            video_path = row['video_path']\n",
    "            if self.is_video_valid(video_path):\n",
    "                valid_indices.append(idx)\n",
    "            \n",
    "            if (idx + 1) % 500 == 0:\n",
    "                print(f\"  Progress: {idx + 1}/{total} files validated...\")\n",
    "        \n",
    "        return df.iloc[valid_indices].reset_index(drop=True)\n",
    "    \n",
    "    def _validate_dataset_lenient(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Lenient validation - just check file existence and basic OpenCV loading\"\"\"\n",
    "        print(\"🔄 Performing lenient validation (file existence + basic OpenCV)...\")\n",
    "        \n",
    "        valid_indices = []\n",
    "        total = len(df)\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            video_path = row['video_path']\n",
    "            \n",
    "            # Lenient criteria: file exists and OpenCV can open it\n",
    "            if (os.path.exists(video_path) and \n",
    "                os.path.getsize(video_path) > 1000 and\n",
    "                self._basic_opencv_check(video_path)):\n",
    "                valid_indices.append(idx)\n",
    "                self.stats['valid'] += 1\n",
    "            else:\n",
    "                self.stats['invalid'] += 1\n",
    "            \n",
    "            self.stats['total'] += 1\n",
    "            \n",
    "            if (idx + 1) % 500 == 0:\n",
    "                print(f\"  Progress: {idx + 1}/{total} files validated...\")\n",
    "        \n",
    "        return df.iloc[valid_indices].reset_index(drop=True)\n",
    "    \n",
    "    def _basic_opencv_check(self, video_path: str) -> bool:\n",
    "        \"\"\"Most basic OpenCV check - just try to open\"\"\"\n",
    "        try:\n",
    "            with ultimate_stderr_suppression():\n",
    "                cap = cv2.VideoCapture(video_path)\n",
    "                is_opened = cap.isOpened()\n",
    "                cap.release()\n",
    "                return is_opened\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def print_stats(self):\n",
    "        \"\"\"Print comprehensive validation statistics\"\"\"\n",
    "        print(f\"\\n📊 Final validation statistics:\")\n",
    "        print(f\"  Total files: {self.stats['total']}\")\n",
    "        print(f\"  ✅ Valid files: {self.stats['valid']}\")\n",
    "        print(f\"  ❌ Invalid files: {self.stats['invalid']}\")\n",
    "        print(f\"  📁 Not found: {self.stats['not_found']}\")\n",
    "        print(f\"  📏 Too small: {self.stats['too_small']}\")\n",
    "        if self.ffprobe_available:\n",
    "            print(f\"  🔧 FFprobe success: {self.stats['ffprobe_success']}\")\n",
    "            print(f\"  🔧 FFprobe failed: {self.stats['ffprobe_failed']}\")\n",
    "        print(f\"  📹 OpenCV success: {self.stats['opencv_success']}\")\n",
    "        print(f\"  📹 OpenCV failed: {self.stats['opencv_failed']}\")\n",
    "        if self.stats['total'] > 0:\n",
    "            print(f\"  📈 Overall success rate: {100 * self.stats['valid'] / self.stats['total']:.1f}%\")\n",
    "\n",
    "# =============================================================================\n",
    "# 2. ROBUST VIDEO LOADER (SAME AS BEFORE)\n",
    "# =============================================================================\n",
    "\n",
    "class RobustVideoLoader:\n",
    "    \"\"\"Robust video loader with better error handling\"\"\"\n",
    "    \n",
    "    def __init__(self, max_frames=8, image_size=224):\n",
    "        self.max_frames = max_frames\n",
    "        self.image_size = image_size\n",
    "        self.video_mean = [0.485, 0.456, 0.406]\n",
    "        self.video_std = [0.229, 0.224, 0.225]\n",
    "        self.successful_loads = 0\n",
    "        self.failed_loads = 0\n",
    "        \n",
    "        # Create diverse fallback videos\n",
    "        self.fallback_videos = self._create_fallback_videos()\n",
    "    \n",
    "    def _create_fallback_videos(self):\n",
    "        \"\"\"Create multiple types of fallback videos\"\"\"\n",
    "        fallbacks = {}\n",
    "        \n",
    "        # Black video\n",
    "        black = torch.zeros(self.max_frames, 3, self.image_size, self.image_size)\n",
    "        fallbacks['black'] = self._normalize_video(black)\n",
    "        \n",
    "        # Noise video\n",
    "        noise = torch.randn(self.max_frames, 3, self.image_size, self.image_size) * 0.2\n",
    "        fallbacks['noise'] = self._normalize_video(noise)\n",
    "        \n",
    "        # Simple pattern\n",
    "        pattern = torch.zeros(self.max_frames, 3, self.image_size, self.image_size)\n",
    "        for t in range(self.max_frames):\n",
    "            for c in range(3):\n",
    "                x = torch.arange(self.image_size).float() / self.image_size\n",
    "                y = torch.arange(self.image_size).float() / self.image_size\n",
    "                xx, yy = torch.meshgrid(x, y, indexing='ij')\n",
    "                pattern[t, c] = (xx + yy + t / self.max_frames) % 1\n",
    "        fallbacks['pattern'] = self._normalize_video(pattern)\n",
    "        \n",
    "        return fallbacks\n",
    "    \n",
    "    def _normalize_video(self, video_tensor):\n",
    "        \"\"\"Apply ImageNet normalization\"\"\"\n",
    "        mean = torch.tensor(self.video_mean).view(1, 3, 1, 1)\n",
    "        std = torch.tensor(self.video_std).view(1, 3, 1, 1)\n",
    "        return (video_tensor - mean) / std\n",
    "    \n",
    "    def load_video_robust(self, video_path: str) -> torch.Tensor:\n",
    "        \"\"\"Load video with comprehensive error handling\"\"\"\n",
    "        try:\n",
    "            with ultimate_stderr_suppression():\n",
    "                cap = None\n",
    "                \n",
    "                # Try multiple backends\n",
    "                for backend in [cv2.CAP_FFMPEG, cv2.CAP_ANY]:\n",
    "                    try:\n",
    "                        cap = cv2.VideoCapture(video_path, backend)\n",
    "                        if cap and cap.isOpened():\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                if not cap or not cap.isOpened():\n",
    "                    self.failed_loads += 1\n",
    "                    return self._get_fallback_video()\n",
    "                \n",
    "                # Get video properties\n",
    "                total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                if total_frames <= 0:\n",
    "                    total_frames = 30  # Assume 30 frames if unknown\n",
    "                \n",
    "                # Sample frames\n",
    "                if total_frames <= self.max_frames:\n",
    "                    frame_indices = list(range(total_frames)) + [total_frames-1] * (self.max_frames - total_frames)\n",
    "                else:\n",
    "                    frame_indices = np.linspace(0, total_frames-1, self.max_frames, dtype=int)\n",
    "                \n",
    "                frames = []\n",
    "                successful_frames = 0\n",
    "                \n",
    "                for idx in frame_indices:\n",
    "                    try:\n",
    "                        cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n",
    "                        ret, frame = cap.read()\n",
    "                        \n",
    "                        if ret and frame is not None and frame.size > 0:\n",
    "                            # Process frame\n",
    "                            frame = cv2.resize(frame, (self.image_size, self.image_size))\n",
    "                            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                            frame = frame.astype(np.float32) / 255.0\n",
    "                            \n",
    "                            # Apply normalization\n",
    "                            mean = np.array(self.video_mean, dtype=np.float32).reshape(1, 1, 3)\n",
    "                            std = np.array(self.video_std, dtype=np.float32).reshape(1, 1, 3)\n",
    "                            frame = (frame - mean) / std\n",
    "                            \n",
    "                            frames.append(frame)\n",
    "                            successful_frames += 1\n",
    "                        else:\n",
    "                            # Use previous frame or create dummy\n",
    "                            if frames:\n",
    "                                frames.append(frames[-1])\n",
    "                            else:\n",
    "                                dummy = np.zeros((self.image_size, self.image_size, 3), dtype=np.float32)\n",
    "                                mean = np.array(self.video_mean, dtype=np.float32).reshape(1, 1, 3)\n",
    "                                std = np.array(self.video_std, dtype=np.float32).reshape(1, 1, 3)\n",
    "                                frames.append((dummy - mean) / std)\n",
    "                    except:\n",
    "                        # Fallback frame\n",
    "                        if frames:\n",
    "                            frames.append(frames[-1])\n",
    "                        else:\n",
    "                            dummy = np.zeros((self.image_size, self.image_size, 3), dtype=np.float32)\n",
    "                            mean = np.array(self.video_mean, dtype=np.float32).reshape(1, 1, 3)\n",
    "                            std = np.array(self.video_std, dtype=np.float32).reshape(1, 1, 3)\n",
    "                            frames.append((dummy - mean) / std)\n",
    "                \n",
    "                cap.release()\n",
    "                \n",
    "                # Ensure we have enough frames\n",
    "                while len(frames) < self.max_frames:\n",
    "                    if frames:\n",
    "                        frames.append(frames[-1])\n",
    "                    else:\n",
    "                        dummy = np.zeros((self.image_size, self.image_size, 3), dtype=np.float32)\n",
    "                        mean = np.array(self.video_mean, dtype=np.float32).reshape(1, 1, 3)\n",
    "                        std = np.array(self.video_std, dtype=np.float32).reshape(1, 1, 3)\n",
    "                        frames.append((dummy - mean) / std)\n",
    "                \n",
    "                # Convert to tensor (T, H, W, C) -> (T, C, H, W)\n",
    "                video_array = np.stack(frames[:self.max_frames])\n",
    "                video_tensor = torch.from_numpy(video_array).float()\n",
    "                video_tensor = video_tensor.permute(0, 3, 1, 2)  # (T, C, H, W)\n",
    "                \n",
    "                # Accept any video that loads (very lenient)\n",
    "                self.successful_loads += 1\n",
    "                return video_tensor\n",
    "                \n",
    "        except:\n",
    "            self.failed_loads += 1\n",
    "            return self._get_fallback_video()\n",
    "    \n",
    "    def _get_fallback_video(self):\n",
    "        \"\"\"Get diverse fallback videos\"\"\"\n",
    "        fallback_types = ['black', 'noise', 'pattern']\n",
    "        fallback_type = fallback_types[self.failed_loads % len(fallback_types)]\n",
    "        return self.fallback_videos[fallback_type].clone()\n",
    "    \n",
    "    def get_stats(self):\n",
    "        total = self.successful_loads + self.failed_loads\n",
    "        if total > 0:\n",
    "            print(f\"📹 Video loading: {self.successful_loads}/{total} successful ({100*self.successful_loads/total:.1f}%)\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3-8. MODEL AND TRAINING CODE (SAME AS BEFORE)\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class SmartMultimodalConfig(PretrainedConfig):\n",
    "    model_type = \"smart_multimodal\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = 4,\n",
    "        text_model_name: str = \"distilbert-base-multilingual-cased\",\n",
    "        video_model_name: str = \"facebook/timesformer-base-finetuned-k400\",\n",
    "        text_hidden_size: int = 768,\n",
    "        video_hidden_size: int = 768,\n",
    "        fusion_hidden_size: int = 256,\n",
    "        classifier_hidden_size: int = 128,\n",
    "        max_frames: int = 8,\n",
    "        image_size: int = 224,\n",
    "        dropout: float = 0.1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_classes = num_classes\n",
    "        self.text_model_name = text_model_name\n",
    "        self.video_model_name = video_model_name\n",
    "        self.text_hidden_size = text_hidden_size\n",
    "        self.video_hidden_size = video_hidden_size\n",
    "        self.fusion_hidden_size = fusion_hidden_size\n",
    "        self.classifier_hidden_size = classifier_hidden_size\n",
    "        self.max_frames = max_frames\n",
    "        self.image_size = image_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "class SmartMultimodalModel(PreTrainedModel):\n",
    "    config_class = SmartMultimodalConfig\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        \n",
    "        print(f\"🔤 Initializing text model: {config.text_model_name}\")\n",
    "        self.text_encoder = AutoModel.from_pretrained(config.text_model_name)\n",
    "        \n",
    "        print(f\"🎬 Initializing video model: {config.video_model_name}\")\n",
    "        self.video_encoder = TimesformerForVideoClassification.from_pretrained(\n",
    "            config.video_model_name,\n",
    "            num_labels=config.num_classes,\n",
    "            ignore_mismatched_sizes=True\n",
    "        )\n",
    "        \n",
    "        # Get actual dimensions\n",
    "        video_dim = self.video_encoder.config.hidden_size\n",
    "        text_dim = self.text_encoder.config.hidden_size\n",
    "        \n",
    "        self.config.video_hidden_size = video_dim\n",
    "        self.config.text_hidden_size = text_dim\n",
    "        \n",
    "        print(f\"📐 Model dimensions - Text: {text_dim}, Video: {video_dim}\")\n",
    "        \n",
    "        # Projection layers\n",
    "        self.text_projection = nn.Sequential(\n",
    "            nn.Linear(text_dim, config.fusion_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "        \n",
    "        self.video_projection = nn.Sequential(\n",
    "            nn.Linear(video_dim, config.fusion_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "        \n",
    "        # Fusion layers\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(config.fusion_hidden_size * 2, config.fusion_hidden_size),\n",
    "            nn.LayerNorm(config.fusion_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(config.fusion_hidden_size, config.classifier_hidden_size),\n",
    "            nn.LayerNorm(config.classifier_hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.dropout)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Linear(config.classifier_hidden_size, config.num_classes)\n",
    "        \n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(f\"🧠 Model: {total_params:,} total, {trainable_params:,} trainable\")\n",
    "    \n",
    "    def forward(self, pixel_values, input_ids, attention_mask, labels=None, **kwargs):\n",
    "        # Text encoding\n",
    "        text_outputs = self.text_encoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        text_features = text_outputs.last_hidden_state[:, 0, :]\n",
    "        text_features = self.text_projection(text_features)\n",
    "        \n",
    "        # Video encoding with silent error handling\n",
    "        try:\n",
    "            # Fix tensor dimensions: (B, T, C, H, W) -> (B, T, C, H, W) (TimeSformer format)\n",
    "            batch_size, num_frames, channels, height, width = pixel_values.shape\n",
    "            \n",
    "            video_outputs = self.video_encoder.timesformer(pixel_values)\n",
    "            video_features = video_outputs.last_hidden_state[:, 0]  # CLS token\n",
    "            \n",
    "        except:\n",
    "            # Silent fallback\n",
    "            batch_size = pixel_values.size(0)\n",
    "            video_features = torch.zeros(\n",
    "                batch_size, self.config.video_hidden_size,\n",
    "                dtype=text_features.dtype, device=text_features.device\n",
    "            )\n",
    "        \n",
    "        video_features = self.video_projection(video_features)\n",
    "        \n",
    "        # Fusion\n",
    "        combined_features = torch.cat([text_features, video_features], dim=1)\n",
    "        fused_features = self.fusion(combined_features)\n",
    "        logits = self.classifier(fused_features)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "            loss = loss_fct(logits, labels)\n",
    "        \n",
    "        return {'loss': loss, 'logits': logits}\n",
    "\n",
    "class SmartDataManager:\n",
    "    def __init__(self, csv_path: str):\n",
    "        self.csv_path = csv_path\n",
    "        self.df = None\n",
    "        self.classes = None\n",
    "        self.class_to_id = None\n",
    "        self.id_to_class = None\n",
    "        self.num_classes = None\n",
    "        \n",
    "    def load_data(self):\n",
    "        print(\"📂 Loading dataset...\")\n",
    "        self.df = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Fix video paths\n",
    "        # self.df['video_path'] = self.df['video_path'].str.replace(\n",
    "        #     r'^/kaggle/working/extra_tikharm/',\n",
    "        #     '/kaggle/input/extra-dataset/',\n",
    "        #     regex=True\n",
    "        # )\n",
    "        \n",
    "        self.df['combined_text'] = self.df['combined_text'].fillna(\"\")\n",
    "        \n",
    "        print(f\"Dataset loaded: {len(self.df)} samples\")\n",
    "        print(f\"Classes distribution:\\n{self.df['class_name'].value_counts()}\")\n",
    "        \n",
    "        self.classes = sorted(self.df['class_name'].unique())\n",
    "        self.class_to_id = {cls: i for i, cls in enumerate(self.classes)}\n",
    "        self.id_to_class = {i: cls for cls, i in self.class_to_id.items()}\n",
    "        self.num_classes = len(self.classes)\n",
    "        \n",
    "        print(f\"📊 Classes ({self.num_classes}): {self.classes}\")\n",
    "        \n",
    "        # Print some sample paths to debug\n",
    "        print(f\"\\n🔍 Sample video paths:\")\n",
    "        for i in range(min(5, len(self.df))):\n",
    "            path = self.df.iloc[i]['video_path']\n",
    "            exists = os.path.exists(path)\n",
    "            print(f\"  {path} - Exists: {exists}\")\n",
    "        \n",
    "    def get_smart_splits(self, validator: SmartVideoValidator):\n",
    "        \"\"\"Get splits with smart validation\"\"\"\n",
    "        print(\"🧹 Creating smart validated splits...\")\n",
    "        \n",
    "        # Get original splits\n",
    "        # train_df = self.df[self.df['split'] == 'train'].reset_index(drop=True)\n",
    "        # val_df = self.df[self.df['split'] == 'val'].reset_index(drop=True)\n",
    "        # test_df = self.df[self.df['split'] == 'test'].reset_index(drop=True)\n",
    "\n",
    "        train_df = self.df[(self.df['split'] == 'train') & (self.df['original_dir'] != 'extra')].reset_index(drop=True)\n",
    "        val_df = self.df[(self.df['split'] == 'val') & (self.df['original_dir'] != 'extra')].reset_index(drop=True)\n",
    "        test_df = self.df[self.df['split'] == 'test'].reset_index(drop=True)\n",
    "        \n",
    "        print(f\"Original splits - Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "        \n",
    "        # Use sample validation first\n",
    "        print(\"Validating training set...\")\n",
    "        clean_train_df = validator.validate_dataset_sample(train_df, sample_size=100)\n",
    "        if len(clean_train_df) < 50:\n",
    "            print(\"⚠️ Too few training samples, using lenient mode for all...\")\n",
    "            clean_train_df = validator._validate_dataset_lenient(train_df)\n",
    "        \n",
    "        print(\"Validating validation set...\")\n",
    "        clean_val_df = validator.validate_dataset_sample(val_df, sample_size=50)\n",
    "        if len(clean_val_df) < 10:\n",
    "            clean_val_df = validator._validate_dataset_lenient(val_df)\n",
    "        \n",
    "        print(\"Validating test set...\")\n",
    "        clean_test_df = validator.validate_dataset_sample(test_df, sample_size=50)\n",
    "        if len(clean_test_df) < 10:\n",
    "            clean_test_df = validator._validate_dataset_lenient(test_df)\n",
    "        \n",
    "        validator.print_stats()\n",
    "        \n",
    "        print(f\"\\n✨ Final splits:\")\n",
    "        print(f\"  🏋️ Train: {len(clean_train_df)} (was {len(train_df)})\")\n",
    "        print(f\"  ✅ Val: {len(clean_val_df)} (was {len(val_df)})\")\n",
    "        print(f\"  🧪 Test: {len(clean_test_df)} (was {len(test_df)})\")\n",
    "        \n",
    "        return clean_train_df, clean_val_df, clean_test_df\n",
    "\n",
    "class SmartDataset(Dataset):\n",
    "    \"\"\"Smart dataset with robust video loading AND TRACKING\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe, text_tokenizer, max_frames=8, image_size=224, max_text_length=64, dataset_name=\"unknown\"):\n",
    "        self.df = dataframe\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.max_frames = max_frames\n",
    "        self.image_size = image_size\n",
    "        self.max_text_length = max_text_length\n",
    "        self.dataset_name = dataset_name  # Track dataset name\n",
    "        \n",
    "        # FIXED: Store actual dataframe length at creation\n",
    "        self.actual_length = len(self.df)\n",
    "        \n",
    "        self.video_loader = RobustVideoLoader(max_frames, image_size)\n",
    "        \n",
    "        print(f\"🛡️ {self.dataset_name} dataset: {self.actual_length} samples (verified)\")\n",
    "        \n",
    "        # Store indices for verification\n",
    "        self.sample_indices = list(self.df.index)\n",
    "        \n",
    "    def __len__(self):\n",
    "        # FIXED: Return actual dataframe length, not processed count\n",
    "        return self.actual_length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            # FIXED: Use actual dataframe index \n",
    "            row = self.df.iloc[idx]\n",
    "            \n",
    "            # Process text\n",
    "            text = str(row['combined_text']) if pd.notna(row['combined_text']) else \"\"\n",
    "            if not text.strip():\n",
    "                text = \"[EMPTY]\"\n",
    "            \n",
    "            text_inputs = self.text_tokenizer(\n",
    "                text,\n",
    "                max_length=self.max_text_length,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            \n",
    "            # Load video robustly\n",
    "            video_path = row['video_path']\n",
    "            pixel_values = self.video_loader.load_video_robust(video_path)\n",
    "            \n",
    "            # Get label\n",
    "            label = data_manager.class_to_id[row['class_name']]\n",
    "            \n",
    "            return {\n",
    "                'pixel_values': pixel_values,\n",
    "                'input_ids': text_inputs['input_ids'].squeeze(0),\n",
    "                'attention_mask': text_inputs['attention_mask'].squeeze(0),\n",
    "                'labels': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error at index {idx} in {self.dataset_name}: {e}\")\n",
    "            \n",
    "            # Safe fallback\n",
    "            dummy_video = torch.zeros(self.max_frames, 3, self.image_size, self.image_size)\n",
    "            dummy_input_ids = torch.zeros(self.max_text_length, dtype=torch.long)\n",
    "            dummy_attention_mask = torch.zeros(self.max_text_length, dtype=torch.long)\n",
    "            \n",
    "            return {\n",
    "                'pixel_values': dummy_video,\n",
    "                'input_ids': dummy_input_ids,\n",
    "                'attention_mask': dummy_attention_mask,\n",
    "                'labels': torch.tensor(0, dtype=torch.long)\n",
    "            }\n",
    "    \n",
    "    def get_stats(self):\n",
    "        print(f\"📊 {self.dataset_name} stats:\")\n",
    "        print(f\"  DataFrame length: {len(self.df)}\")\n",
    "        print(f\"  Actual length: {self.actual_length}\")\n",
    "        self.video_loader.get_stats()\n",
    "\n",
    "def compute_comprehensive_metrics(eval_pred: EvalPrediction) -> Dict[str, float]:\n",
    "    \"\"\"Compute all 4 key metrics\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    labels = eval_pred.label_ids\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average='macro', zero_division=0)\n",
    "    recall = recall_score(labels, predictions, average='macro', zero_division=0)\n",
    "    precision = precision_score(labels, predictions, average='macro', zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'recall': recall,\n",
    "        'precision': precision\n",
    "    }\n",
    "\n",
    "def smart_collate_fn(batch):\n",
    "    \"\"\"Smart collate function\"\"\"\n",
    "    try:\n",
    "        return {\n",
    "            'pixel_values': torch.stack([item['pixel_values'] for item in batch]),\n",
    "            'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "            'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
    "            'labels': torch.stack([item['labels'] for item in batch])\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Collate error: {e}\")\n",
    "        batch_size = len(batch)\n",
    "        return {\n",
    "            'pixel_values': torch.zeros(batch_size, 8, 3, 224, 224),\n",
    "            'input_ids': torch.zeros(batch_size, 64, dtype=torch.long),\n",
    "            'attention_mask': torch.zeros(batch_size, 64, dtype=torch.long),\n",
    "            'labels': torch.zeros(batch_size, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN EXECUTION WITH FIXED DATA TRACKING\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training function with FIXED data counting\"\"\"\n",
    "    \n",
    "    try:\n",
    "        print(\"🚀 Starting smart adaptive training (FIXED VERSION)...\")\n",
    "        \n",
    "        # Initialize data manager\n",
    "        global data_manager\n",
    "        data_manager = SmartDataManager('/kaggle/input/processed-data/tikharm_vi_en_complete.csv')\n",
    "        data_manager.load_data()\n",
    "        \n",
    "        # Initialize smart validator (not strict)\n",
    "        print(\"🔍 Initializing smart video validator...\")\n",
    "        validator = SmartVideoValidator(strict_mode=False)\n",
    "        \n",
    "        # Get smart validated data\n",
    "        train_df, val_df, test_df = data_manager.get_smart_splits(validator)\n",
    "        \n",
    "        # VERIFY DATA SPLITS AGAIN\n",
    "        print(f\"\\n🔍 VERIFICATION - Actual dataframe sizes:\")\n",
    "        print(f\"  Train DF: {len(train_df)} rows\")\n",
    "        print(f\"  Val DF: {len(val_df)} rows\") \n",
    "        print(f\"  Test DF: {len(test_df)} rows\")\n",
    "        \n",
    "        # Check data sufficiency\n",
    "        if len(train_df) < 50:\n",
    "            print(\"⚠️ Very few training samples!\")\n",
    "            print(\"🔄 Proceeding with available data...\")\n",
    "        \n",
    "        # Initialize models\n",
    "        print(\"🧠 Initializing models...\")\n",
    "        text_model_name = \"distilbert-base-multilingual-cased\"\n",
    "        text_tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n",
    "        \n",
    "        config = SmartMultimodalConfig(\n",
    "            num_classes=data_manager.num_classes,\n",
    "            text_model_name=text_model_name,\n",
    "            max_frames=8,\n",
    "            image_size=224,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"💻 Using device: {device}\")\n",
    "        \n",
    "        model = SmartMultimodalModel(config)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Create smart datasets WITH PROPER TRACKING\n",
    "        print(\"📊 Creating smart datasets with tracking...\")\n",
    "        train_dataset = SmartDataset(train_df, text_tokenizer, dataset_name=\"TRAIN\")\n",
    "        val_dataset = SmartDataset(val_df, text_tokenizer, dataset_name=\"VAL\")\n",
    "        test_dataset = SmartDataset(test_df, text_tokenizer, dataset_name=\"TEST\")\n",
    "        \n",
    "        # VERIFY DATASET LENGTHS\n",
    "        print(f\"\\n🎯 FINAL DATASET VERIFICATION:\")\n",
    "        print(f\"  Train dataset: {len(train_dataset)} samples\")\n",
    "        print(f\"  Val dataset: {len(val_dataset)} samples\")\n",
    "        print(f\"  Test dataset: {len(test_dataset)} samples\")\n",
    "        \n",
    "        # Test sample loading\n",
    "        print(\"🧪 Testing sample loading...\")\n",
    "        sample = train_dataset[0]\n",
    "        print(f\"  ✅ Video shape: {sample['pixel_values'].shape}\")\n",
    "        print(f\"  ✅ Text shape: {sample['input_ids'].shape}\")\n",
    "        \n",
    "        # Test forward pass\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            batch = {k: v.unsqueeze(0).to(device) for k, v in sample.items()}\n",
    "            output = model(**batch)\n",
    "            print(f\"  ✅ Output shape: {output['logits'].shape}\")\n",
    "        \n",
    "        # Training setup with FIXED batch settings\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir='./smart-multimodal-fixed',\n",
    "            num_train_epochs=6,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            gradient_accumulation_steps=2,\n",
    "            learning_rate=1e-4,\n",
    "            weight_decay=0.01,\n",
    "            warmup_steps=100,\n",
    "            logging_steps=50,\n",
    "            eval_strategy=\"steps\",\n",
    "            eval_steps=50,\n",
    "            save_strategy=\"steps\",\n",
    "            save_steps=50,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"f1_score\",\n",
    "            fp16=True,\n",
    "            dataloader_num_workers=0,\n",
    "            remove_unused_columns=False,\n",
    "            report_to=None,\n",
    "            save_total_limit=2,\n",
    "            # FIXED: Don't drop last to preserve data count\n",
    "            dataloader_drop_last=False,  # Changed from True to False\n",
    "            max_grad_norm=1.0,\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            data_collator=smart_collate_fn,\n",
    "            compute_metrics=compute_comprehensive_metrics,\n",
    "            tokenizer=text_tokenizer,\n",
    "        )\n",
    "        \n",
    "        # Start training\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"🚀 STARTING SMART ADAPTIVE TRAINING (FIXED)!\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Train with error suppression\n",
    "        with ultimate_stderr_suppression():\n",
    "            train_result = trainer.train()\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\n🎉 TRAINING COMPLETED in {training_time:.1f}s!\")\n",
    "        \n",
    "        # Save model\n",
    "        trainer.save_model(\"./final-smart-model-fixed\")\n",
    "        print(\"💾 Model saved!\")\n",
    "        \n",
    "        # Evaluation\n",
    "        print(\"📊 Running evaluation...\")\n",
    "        eval_results = trainer.evaluate()\n",
    "        \n",
    "        print(f\"\\n🎯 VALIDATION RESULTS:\")\n",
    "        print(f\"  Accuracy: {eval_results['eval_accuracy']:.4f} ({eval_results['eval_accuracy']*100:.2f}%)\")\n",
    "        print(f\"  F1-Score: {eval_results['eval_f1_score']:.4f}\")\n",
    "        print(f\"  Recall: {eval_results['eval_recall']:.4f}\")\n",
    "        print(f\"  Precision: {eval_results['eval_precision']:.4f}\")\n",
    "        \n",
    "        # FIXED Test evaluation with manual prediction\n",
    "        print(\"🧪 Running FIXED test evaluation...\")\n",
    "        \n",
    "        # Use manual DataLoader to control exact sample count\n",
    "        test_dataloader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=8, \n",
    "            shuffle=False, \n",
    "            drop_last=False,  # Don't drop any samples\n",
    "            collate_fn=smart_collate_fn\n",
    "        )\n",
    "        \n",
    "        model.eval()\n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        \n",
    "        print(f\"🔍 Processing test data in {len(test_dataloader)} batches...\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(test_dataloader):\n",
    "                # Move to device\n",
    "                batch = {k: v.to(device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "                \n",
    "                # Get predictions\n",
    "                outputs = model(**batch)\n",
    "                predictions = torch.argmax(outputs['logits'], dim=1)\n",
    "                \n",
    "                all_predictions.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(batch['labels'].cpu().numpy())\n",
    "                \n",
    "                if (batch_idx + 1) % 10 == 0:\n",
    "                    print(f\"  Processed batch {batch_idx + 1}/{len(test_dataloader)}\")\n",
    "        \n",
    "        # Convert to numpy arrays\n",
    "        y_pred = np.array(all_predictions)\n",
    "        y_true = np.array(all_labels)\n",
    "        \n",
    "        print(f\"\\n🎯 FINAL TEST DATA VERIFICATION:\")\n",
    "        print(f\"  Expected test samples: {len(test_dataset)}\")\n",
    "        print(f\"  Actual predictions: {len(y_pred)}\")\n",
    "        print(f\"  Actual labels: {len(y_true)}\")\n",
    "        print(f\"  Data consistency: {'✅ CONSISTENT' if len(y_pred) == len(test_dataset) else '❌ INCONSISTENT'}\")\n",
    "        \n",
    "        # Test metrics\n",
    "        test_accuracy = accuracy_score(y_true, y_pred)\n",
    "        test_f1 = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        test_recall = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        test_precision = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "        \n",
    "        print(f\"\\n🏆 FINAL TEST RESULTS (VERIFIED):\")\n",
    "        print(f\"  Test samples: {len(y_true)}\")\n",
    "        print(f\"  Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "        print(f\"  F1-Score: {test_f1:.4f}\")\n",
    "        print(f\"  Recall: {test_recall:.4f}\")\n",
    "        print(f\"  Precision: {test_precision:.4f}\")\n",
    "        \n",
    "        # Classification report\n",
    "        print(f\"\\n📋 CLASSIFICATION REPORT (VERIFIED):\")\n",
    "        print(\"-\" * 60)\n",
    "        class_report = classification_report(\n",
    "            y_true, y_pred, \n",
    "            target_names=data_manager.classes,\n",
    "            digits=4\n",
    "        )\n",
    "        print(class_report)\n",
    "        \n",
    "        # Save results\n",
    "        import json\n",
    "        results = {\n",
    "            'model_type': 'Smart_Adaptive_Multimodal_FIXED',\n",
    "            'data_verification': {\n",
    "                'train_samples': len(train_dataset),\n",
    "                'val_samples': len(val_dataset),\n",
    "                'test_samples': len(test_dataset),\n",
    "                'test_predictions': len(y_pred),\n",
    "                'data_consistent': len(y_pred) == len(test_dataset)\n",
    "            },\n",
    "            'validation_metrics': {\n",
    "                'accuracy': eval_results['eval_accuracy'],\n",
    "                'f1_score': eval_results['eval_f1_score'],\n",
    "                'recall': eval_results['eval_recall'],\n",
    "                'precision': eval_results['eval_precision']\n",
    "            },\n",
    "            'test_metrics': {\n",
    "                'accuracy': test_accuracy,\n",
    "                'f1_score': test_f1,\n",
    "                'recall': test_recall,\n",
    "                'precision': test_precision\n",
    "            },\n",
    "            'training_time_seconds': training_time,\n",
    "            'validation_stats': validator.stats,\n",
    "            'dataset_sizes': {\n",
    "                'train': len(train_dataset),\n",
    "                'val': len(val_dataset),\n",
    "                'test': len(test_dataset)\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open('smart_results_fixed.json', 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        print(\"💾 Results saved!\")\n",
    "        \n",
    "        # Final statistics\n",
    "        print(\"\\n📈 Final statistics:\")\n",
    "        train_dataset.get_stats()\n",
    "        val_dataset.get_stats() \n",
    "        test_dataset.get_stats()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"🎊 SMART TRAINING COMPLETED SUCCESSFULLY (FIXED)!\")\n",
    "        print(\"✅ DATA COUNTING ISSUES RESOLVED!\")\n",
    "        print(\"📊 ALL METRICS CALCULATED CORRECTLY!\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Unexpected error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        cleanup_memory()\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d28c0cc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T23:42:04.937606Z",
     "iopub.status.busy": "2025-07-01T23:42:04.937369Z",
     "iopub.status.idle": "2025-07-01T23:42:04.948770Z",
     "shell.execute_reply": "2025-07-01T23:42:04.948197Z"
    },
    "papermill": {
     "duration": 0.022621,
     "end_time": "2025-07-01T23:42:04.949873",
     "exception": false,
     "start_time": "2025-07-01T23:42:04.927252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import cv2\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from transformers import (\n",
    "#     AutoTokenizer, AutoModel, AutoImageProcessor,\n",
    "#     TimesformerForVideoClassification, TimesformerConfig,\n",
    "#     TrainingArguments, Trainer\n",
    "# )\n",
    "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import gc\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# df = pd.read_csv('/kaggle/input/processed-data/tikharm_vi_en_complete.csv')\n",
    "# print(f\"Dataset shape: {df.shape}\")\n",
    "# print(f\"Classes: {df['class_name'].value_counts()}\")\n",
    "# print(f\"Splits: {df['split'].value_counts()}\")\n",
    "\n",
    "# df['combined_text'] = df['combined_text'].fillna(\"\")\n",
    "\n",
    "# classes = sorted(df['class_name'].unique())\n",
    "# label2id = {label: i for i, label in enumerate(classes)}\n",
    "# id2label = {i: label for i, label in label2id.items()}\n",
    "# num_classes = len(classes)\n",
    "\n",
    "# print(f\"Number of classes: {num_classes}\")\n",
    "# print(f\"Classes: {classes}\")\n",
    "\n",
    "# train_df = df[df['split'] == 'train'].reset_index(drop=True)\n",
    "# val_df = df[df['split'] == 'val'].reset_index(drop=True)\n",
    "# test_df = df[df['split'] == 'test'].reset_index(drop=True)\n",
    "\n",
    "# print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "# class TimeSformerMultimodalModel(nn.Module):\n",
    "#     def __init__(self, num_classes, \n",
    "#                  text_model_name=\"distilbert-base-multilingual-cased\",\n",
    "#                  video_model_name=\"facebook/timesformer-base-finetuned-k400\"):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.text_encoder = AutoModel.from_pretrained(text_model_name)\n",
    "#         text_dim = self.text_encoder.config.hidden_size\n",
    "        \n",
    "#         self.timesformer_config = TimesformerConfig(\n",
    "#             image_size=112,                    # Giảm từ 224 -> 112 để tiết kiệm memory\n",
    "#             patch_size=16,                     # Giữ nguyên\n",
    "#             num_channels=3,\n",
    "#             num_frames=8,                      # Tăng từ 4 -> 8 frames để có thông tin tốt hơn\n",
    "#             hidden_size=384,                   # Giảm từ 768 -> 384 để tiết kiệm memory\n",
    "#             num_hidden_layers=6,               # Giảm từ 12 -> 6 layers\n",
    "#             num_attention_heads=6,             # Giảm từ 12 -> 6 heads\n",
    "#             intermediate_size=1536,            # Giảm từ 3072 -> 1536\n",
    "#             hidden_dropout_prob=0.1,\n",
    "#             attention_probs_dropout_prob=0.1,\n",
    "#             attention_type=\"divided_space_time\", # Tối ưu nhất theo paper\n",
    "#             drop_path_rate=0.1,\n",
    "#             num_labels=num_classes,\n",
    "#             qkv_bias=True\n",
    "#         )\n",
    "        \n",
    "#         print(\"Loading TimeSformer...\")\n",
    "#         try:\n",
    "#             self.video_encoder = TimesformerForVideoClassification(self.timesformer_config)\n",
    "            \n",
    "#             pretrained_model = TimesformerForVideoClassification.from_pretrained(\n",
    "#                 video_model_name, \n",
    "#                 ignore_mismatched_sizes=True\n",
    "#             )\n",
    "            \n",
    "#             self._transfer_weights(pretrained_model, self.video_encoder)\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Warning: Could not load pretrained weights: {e}\")\n",
    "#             self.video_encoder = TimesformerForVideoClassification(self.timesformer_config)\n",
    "        \n",
    "#         video_dim = self.timesformer_config.hidden_size\n",
    "        \n",
    "#         self.fusion = nn.Sequential(\n",
    "#             nn.Linear(text_dim + video_dim, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.ReLU(), \n",
    "#             nn.Dropout(0.2),\n",
    "#             nn.Linear(128, num_classes)\n",
    "#         )\n",
    "        \n",
    "#     def _transfer_weights(self, source_model, target_model):\n",
    "#         source_dict = source_model.state_dict()\n",
    "#         target_dict = target_model.state_dict()\n",
    "        \n",
    "#         transferred = 0\n",
    "#         for name, param in target_dict.items():\n",
    "#             if name in source_dict and source_dict[name].shape == param.shape:\n",
    "#                 target_dict[name] = source_dict[name]\n",
    "#                 transferred += 1\n",
    "        \n",
    "#         target_model.load_state_dict(target_dict)\n",
    "#         print(f\"Transferred {transferred}/{len(target_dict)} layers from pretrained model\")\n",
    "        \n",
    "#     def forward(self, input_ids, attention_mask, pixel_values, labels=None):\n",
    "#         text_output = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         text_features = text_output.last_hidden_state.mean(dim=1)\n",
    "        \n",
    "#         video_output = self.video_encoder.timesformer(pixel_values)\n",
    "        \n",
    "#         video_features = video_output.last_hidden_state[:, 0]  # [batch_size, hidden_size]\n",
    "        \n",
    "#         combined = torch.cat([text_features, video_features], dim=1)\n",
    "#         logits = self.fusion(combined)\n",
    "        \n",
    "#         loss = None\n",
    "#         if labels is not None:\n",
    "#             loss = nn.CrossEntropyLoss()(logits, labels)\n",
    "            \n",
    "#         return {'loss': loss, 'logits': logits}\n",
    "\n",
    "\n",
    "# def timesformer_video_loader(video_path, target_frames=8, img_size=112):\n",
    "#     \"\"\"Load video compatible với TimeSformer\"\"\"\n",
    "#     try:\n",
    "#         cap = cv2.VideoCapture(video_path)\n",
    "#         frames = []\n",
    "        \n",
    "#         total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#         if total_frames == 0:\n",
    "#             cap.release()\n",
    "#             return torch.zeros(3, target_frames, img_size, img_size)\n",
    "            \n",
    "#         # Sample frames uniformly\n",
    "#         if total_frames < target_frames:\n",
    "#             # Repeat frames if video is too short\n",
    "#             indices = np.linspace(0, max(0, total_frames-1), target_frames, dtype=int)\n",
    "#         else:\n",
    "#             # Sample uniformly\n",
    "#             indices = np.linspace(0, total_frames-1, target_frames, dtype=int)\n",
    "        \n",
    "#         for idx in indices:\n",
    "#             cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "#             ret, frame = cap.read()\n",
    "#             if ret:\n",
    "#                 # Resize và normalize như TimeSformer expects\n",
    "#                 frame = cv2.resize(frame, (img_size, img_size))\n",
    "#                 frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "#                 frame = frame.astype(np.float32) / 255.0\n",
    "#                 frames.append(frame)\n",
    "#             else:\n",
    "#                 # Dummy frame\n",
    "#                 frames.append(np.zeros((img_size, img_size, 3), dtype=np.float32))\n",
    "                \n",
    "#         cap.release()\n",
    "        \n",
    "#         # Convert to tensor: (T, H, W, C) -> (C, T, H, W)\n",
    "#         if len(frames) == target_frames:\n",
    "#             video_tensor = torch.from_numpy(np.array(frames))\n",
    "#             video_tensor = video_tensor.permute(3, 0, 1, 2)\n",
    "#             return video_tensor\n",
    "#         else:\n",
    "#             return torch.zeros(3, target_frames, img_size, img_size)\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         return torch.zeros(3, target_frames, img_size, img_size)\n",
    "\n",
    "# class TimeSformerDataset(Dataset):\n",
    "#     def __init__(self, dataframe, tokenizer, image_processor=None, \n",
    "#                  max_length=64, img_size=112, num_frames=8):\n",
    "#         self.df = dataframe\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.image_processor = image_processor\n",
    "#         self.max_length = max_length\n",
    "#         self.img_size = img_size\n",
    "#         self.num_frames = num_frames\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.df)\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         row = self.df.iloc[idx]\n",
    "        \n",
    "#         # Text\n",
    "#         text = str(row['combined_text']) if pd.notna(row['combined_text']) else \"\"\n",
    "#         text_inputs = self.tokenizer(\n",
    "#             text,\n",
    "#             max_length=self.max_length,\n",
    "#             padding='max_length', \n",
    "#             truncation=True,\n",
    "#             return_tensors='pt'\n",
    "#         )\n",
    "        \n",
    "#         # Video\n",
    "#         video_path = row['video_path']\n",
    "#         if pd.notna(video_path) and os.path.exists(video_path):\n",
    "#             pixel_values = timesformer_video_loader(video_path, self.num_frames, self.img_size)\n",
    "#         else:\n",
    "#             pixel_values = torch.zeros(3, self.num_frames, self.img_size, self.img_size)\n",
    "        \n",
    "#         if self.image_processor is not None:\n",
    "#             try:\n",
    "#                 video_np = pixel_values.permute(1, 2, 3, 0).numpy()  # (T, H, W, C)\n",
    "#                 video_list = [video_np[i] for i in range(self.num_frames)]\n",
    "                \n",
    "#                 processed = self.image_processor(video_list, return_tensors=\"pt\")\n",
    "#                 pixel_values = processed['pixel_values'].squeeze(0)  # Remove batch dim\n",
    "#             except:\n",
    "#                 mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1, 1)\n",
    "#                 std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1, 1)\n",
    "#                 pixel_values = (pixel_values - mean) / std\n",
    "        \n",
    "#         # Label\n",
    "#         label = label2id[row['class_name']]\n",
    "        \n",
    "#         return {\n",
    "#             'input_ids': text_inputs['input_ids'].squeeze(0),\n",
    "#             'attention_mask': text_inputs['attention_mask'].squeeze(0),\n",
    "#             'pixel_values': pixel_values,\n",
    "#             'labels': torch.tensor(label, dtype=torch.long)\n",
    "#         }\n",
    "\n",
    "# print(\"Initializing models...\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n",
    "\n",
    "# try:\n",
    "#     image_processor = AutoImageProcessor.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n",
    "#     print(\"Loaded TimeSformer image processor\")\n",
    "# except:\n",
    "#     image_processor = None\n",
    "#     print(\"Could not load image processor, using manual normalization\")\n",
    "\n",
    "# model = TimeSformerMultimodalModel(num_classes=num_classes)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# print(f\"Model loaded on: {device}\")\n",
    "# print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# print(\"Creating datasets...\")\n",
    "# train_dataset = TimeSformerDataset(\n",
    "#     train_df, tokenizer, image_processor,\n",
    "#     max_length=64, img_size=112, num_frames=8\n",
    "# )\n",
    "\n",
    "# val_dataset = TimeSformerDataset(\n",
    "#     val_df, tokenizer, image_processor,\n",
    "#     max_length=64, img_size=112, num_frames=8\n",
    "# )\n",
    "\n",
    "# test_dataset = TimeSformerDataset(\n",
    "#     test_df, tokenizer, image_processor,\n",
    "#     max_length=64, img_size=112, num_frames=8\n",
    "# )\n",
    "\n",
    "# print(f\"Train samples: {len(train_dataset)}\")\n",
    "# print(f\"Val samples: {len(val_dataset)}\")\n",
    "# print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# def collate_fn(batch):\n",
    "#     try:\n",
    "#         return {\n",
    "#             'input_ids': torch.stack([item['input_ids'] for item in batch]),\n",
    "#             'attention_mask': torch.stack([item['attention_mask'] for item in batch]),\n",
    "#             'pixel_values': torch.stack([item['pixel_values'] for item in batch]),\n",
    "#             'labels': torch.stack([item['labels'] for item in batch])\n",
    "#         }\n",
    "#     except Exception as e:\n",
    "#         print(f\"Collate error: {e}\")\n",
    "#         batch_size = len(batch)\n",
    "#         return {\n",
    "#             'input_ids': torch.zeros(batch_size, 64, dtype=torch.long),\n",
    "#             'attention_mask': torch.zeros(batch_size, 64, dtype=torch.long),\n",
    "#             'pixel_values': torch.zeros(batch_size, 3, 8, 112, 112),\n",
    "#             'labels': torch.zeros(batch_size, dtype=torch.long)\n",
    "#         }\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "#     return {'accuracy': accuracy_score(eval_pred.label_ids, predictions)}\n",
    "\n",
    "# # Training arguments\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./timesformer-multimodal\",\n",
    "#     num_train_epochs=5,\n",
    "#     per_device_train_batch_size=2,    # Small batch for TimeSformer\n",
    "#     per_device_eval_batch_size=2,\n",
    "#     gradient_accumulation_steps=8,    # Effective batch size = 16\n",
    "#     learning_rate=1e-5,               # Lower LR for pretrained TimeSformer\n",
    "#     weight_decay=0.01,\n",
    "#     warmup_steps=200,\n",
    "#     logging_steps=50,                 # Log mỗi 50 steps\n",
    "#     eval_strategy=\"steps\",            # Thay đổi từ \"epoch\" -> \"steps\"\n",
    "#     eval_steps=50,                    # Đánh giá mỗi 50 steps\n",
    "#     save_strategy=\"steps\",            # Thay đổi từ \"epoch\" -> \"steps\"\n",
    "#     save_steps=50,                    # Save mỗi 50 steps\n",
    "#     load_best_model_at_end=True,\n",
    "#     metric_for_best_model=\"accuracy\",\n",
    "#     fp16=True,\n",
    "#     dataloader_num_workers=0,\n",
    "#     remove_unused_columns=False,\n",
    "#     report_to=\"none\",\n",
    "#     save_total_limit=3,               # Tăng từ 1 -> 3 để lưu nhiều checkpoint hơn\n",
    "#     max_grad_norm=1.0,\n",
    "#     dataloader_drop_last=True,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset,\n",
    "#     data_collator=collate_fn,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "\n",
    "# try:\n",
    "#     sample = train_dataset[0]\n",
    "#     print(\"Sample shapes:\")\n",
    "#     print(f\"  Input IDs: {sample['input_ids'].shape}\")\n",
    "#     print(f\"  Pixel values: {sample['pixel_values'].shape}\")\n",
    "#     print(f\"  Label: {sample['labels']}\")\n",
    "    \n",
    "#     # Test forward pass\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         batch = {k: v.unsqueeze(0).to(device) for k, v in sample.items()}\n",
    "#         output = model(**batch)\n",
    "#         print(f\"  Output logits shape: {output['logits'].shape}\")\n",
    "    \n",
    "#     print(\"Sample loading and forward pass successful!\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Sample test failed: {e}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()\n",
    "\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "# training_successful = False\n",
    "# try:\n",
    "#     print(\"Training with TimeSformer...\")\n",
    "#     train_result = trainer.train()\n",
    "#     print(\"Training completed!\")\n",
    "    \n",
    "#     # Save model\n",
    "#     trainer.save_model(\"./final-timesformer-multimodal\")\n",
    "#     print(\"Model saved!\")\n",
    "    \n",
    "#     # Validation evaluation\n",
    "#     eval_results = trainer.evaluate()\n",
    "#     print(f\"Validation accuracy: {eval_results['eval_accuracy']:.4f}\")\n",
    "#     print(f\"Validation loss: {eval_results['eval_loss']:.4f}\")\n",
    "    \n",
    "#     training_successful = True\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"Training error: {e}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "# print(\"TIMESFORMER MULTIMODAL TEST EVALUATION\")\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# if training_successful:\n",
    "#     try:\n",
    "#         print(\"Predicting on full test set...\")\n",
    "#         test_results = trainer.predict(test_dataset)\n",
    "        \n",
    "#         y_pred = np.argmax(test_results.predictions, axis=1)\n",
    "#         y_true = test_results.label_ids\n",
    "        \n",
    "#         test_accuracy = accuracy_score(y_true, y_pred)\n",
    "#         print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
    "#         print(f\"Test Loss: {test_results.metrics['test_loss']:.4f}\")\n",
    "        \n",
    "#         # Classification report\n",
    "#         print(\"\\nClassification Report:\")\n",
    "#         print(\"-\" * 60)\n",
    "#         class_report = classification_report(\n",
    "#             y_true, y_pred, \n",
    "#             target_names=classes,\n",
    "#             digits=4\n",
    "#         )\n",
    "#         print(class_report)\n",
    "        \n",
    "#         # Confusion Matrix\n",
    "#         cm = confusion_matrix(y_true, y_pred)\n",
    "#         print(\"\\nConfusion Matrix:\")\n",
    "#         print(\"-\" * 40)\n",
    "#         header = \"\".join([f\"{cls[:10]:>10}\" for cls in classes])\n",
    "#         print(f\"{'':>15}{header}\")\n",
    "        \n",
    "#         for i, actual_class in enumerate(classes):\n",
    "#             row = \"\".join([f\"{cm[i,j]:>10}\" for j in range(len(classes))])\n",
    "#             print(f\"{actual_class[:13]:>13}: {row}\")\n",
    "        \n",
    "#         # Performance summary\n",
    "#         print(\"\\n\" + \"=\" * 50)\n",
    "#         print(\"TIMESFORMER MULTIMODAL PERFORMANCE\")\n",
    "#         print(\"=\" * 50)\n",
    "#         print(f\"Architecture: TimeSformer + DistilBERT\")\n",
    "#         print(f\"Video frames: 8\")\n",
    "#         print(f\"Image size: 112x112\")\n",
    "#         print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "#         print(f\"Model Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "        \n",
    "#         # Save results\n",
    "#         import json\n",
    "#         results = {\n",
    "#             'model_type': 'TimeSformer_Multimodal',\n",
    "#             'test_accuracy': test_accuracy,\n",
    "#             'test_loss': test_results.metrics['test_loss'],\n",
    "#             'classification_report': class_report,\n",
    "#             'confusion_matrix': cm.tolist(),\n",
    "#             'classes': classes,\n",
    "#             'model_config': {\n",
    "#                 'video_frames': 8,\n",
    "#                 'image_size': 112,\n",
    "#                 'hidden_size': 384,\n",
    "#                 'num_layers': 6,\n",
    "#                 'attention_heads': 6\n",
    "#             }\n",
    "#         }\n",
    "        \n",
    "#         with open('timesformer_results.json', 'w') as f:\n",
    "#             json.dump(results, f, indent=2)\n",
    "        \n",
    "#         print(\"Results saved to timesformer_results.json\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Testing error: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 60)\n",
    "# print(\"TIMESFORMER EXPERIMENT COMPLETE!\")\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195c1aa9",
   "metadata": {
    "papermill": {
     "duration": 0.007469,
     "end_time": "2025-07-01T23:42:04.965341",
     "exception": false,
     "start_time": "2025-07-01T23:42:04.957872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e6e1ea",
   "metadata": {
    "papermill": {
     "duration": 0.007431,
     "end_time": "2025-07-01T23:42:04.980755",
     "exception": false,
     "start_time": "2025-07-01T23:42:04.973324",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5303541,
     "sourceId": 8816398,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7765310,
     "sourceId": 12319530,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7771839,
     "sourceId": 12329067,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26856.579517,
   "end_time": "2025-07-01T23:42:08.312491",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-01T16:14:31.732974",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "045ca771ec3844cab6df46f09734f96f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c4229f32cf8e485382702c0c57c253be",
        "IPY_MODEL_2913bfba5d534d9e8e1e23334e4857e4",
        "IPY_MODEL_c02537cc628b4acf8991f0d0baa6e031"
       ],
       "layout": "IPY_MODEL_f5f46cf442fd43969493fad397cc7a11",
       "tabbable": null,
       "tooltip": null
      }
     },
     "0742dda7ce52495098e570e238e0e5b7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a6aa787459b142c9971ce2f42be8f827",
       "placeholder": "​",
       "style": "IPY_MODEL_8084979977b2415082441f1263c48cb2",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json: 100%"
      }
     },
     "0834c16064ed4bdc8706be3bc3198bb6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_49d865a31b0b46aab90bfd36c9f23387",
        "IPY_MODEL_4a4621f2564b473d94472dbeecab2dfc",
        "IPY_MODEL_33bbf24d52634b1aa770bae0903f8528"
       ],
       "layout": "IPY_MODEL_f9ef44b3b69b4736a865195cbd7a7d84",
       "tabbable": null,
       "tooltip": null
      }
     },
     "08abac3041af493085f362f76d44bd59": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "0d319f30536642eeab09ee599ac10d74": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "10f619ea10154149ba58578d9fce7e77": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "189a6bf693614aa6a292f31351d6ae4a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1fd43d4e688e41db8bd37ed519702c02": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c82b5c4f065749e4861b7eee17fb3b68",
       "placeholder": "​",
       "style": "IPY_MODEL_8e42b5d6d03b4139acd8522f2afc5f8e",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer.json: 100%"
      }
     },
     "21f548adaba844178d8eaf0a4cc0ec76": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2362f9367a5f4a0cb4a86550389eead2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "20px"
      }
     },
     "28dc9b4b2221457ab528143ac162fea3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e739366c152041eab509fe88c9ba8fc9",
       "max": 1961828,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3e90bb228d1048288b211bca205882ff",
       "tabbable": null,
       "tooltip": null,
       "value": 1961828
      }
     },
     "2913bfba5d534d9e8e1e23334e4857e4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e3878f0da90d44d58533edfceaa72763",
       "max": 995526,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_65815a61df3e4afab26b0ef5d9267211",
       "tabbable": null,
       "tooltip": null,
       "value": 995526
      }
     },
     "2cb21c81c9d94b6f8494830cc710132c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5f7fd738283141618b705900a0ed18b9",
       "placeholder": "​",
       "style": "IPY_MODEL_3b877fd743334b919d43d440a264c2f8",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors: 100%"
      }
     },
     "314db1afbda148a5813b016b8b8431f3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "32467d877eea4951baf4755598b4a3d9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9cf72d140fb04d09bcb0e51e30cdddcf",
       "placeholder": "​",
       "style": "IPY_MODEL_08abac3041af493085f362f76d44bd59",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors: 100%"
      }
     },
     "33bbf24d52634b1aa770bae0903f8528": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_bbff34a848dc4ccc80b7ab54d216ba30",
       "placeholder": "​",
       "style": "IPY_MODEL_189a6bf693614aa6a292f31351d6ae4a",
       "tabbable": null,
       "tooltip": null,
       "value": " 486M/486M [00:02&lt;00:00, 410MB/s]"
      }
     },
     "3b877fd743334b919d43d440a264c2f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3c2d589d0ee947a2994a00424b9e3335": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3e90bb228d1048288b211bca205882ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "40317f1455ae4f2289446c92b7895443": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "43977cc3a97247ec869c94dff6a8aece": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "4493fee9b5214c1dbb0f20dd8cd31132": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_40317f1455ae4f2289446c92b7895443",
       "max": 486296528,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e89aaac635f440fbbf35e8011db0d0fa",
       "tabbable": null,
       "tooltip": null,
       "value": 486296528
      }
     },
     "458d2337a752418888d532a2bda15c88": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "492aceb73fe4478ca920faa87a0f7db3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_2362f9367a5f4a0cb4a86550389eead2",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6592ec83f98b469dbd6a4d0b62e0a4c9",
       "tabbable": null,
       "tooltip": null,
       "value": 1
      }
     },
     "49d865a31b0b46aab90bfd36c9f23387": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_50f9a151428d48f1a21216e9bb55945d",
       "placeholder": "​",
       "style": "IPY_MODEL_f715c94b6a1c4adfb0db01864134bd68",
       "tabbable": null,
       "tooltip": null,
       "value": "pytorch_model.bin: 100%"
      }
     },
     "4a4621f2564b473d94472dbeecab2dfc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_64aab57ec23e4795aca104534757ba15",
       "max": 486348721,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_59005a407c63499b955332e0b139fa7a",
       "tabbable": null,
       "tooltip": null,
       "value": 486348721
      }
     },
     "4e052da5e91c4a5684a04f26d69026ed": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "50f9a151428d48f1a21216e9bb55945d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "540637dcf2654bb79936dfbe3e68c983": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_32467d877eea4951baf4755598b4a3d9",
        "IPY_MODEL_4493fee9b5214c1dbb0f20dd8cd31132",
        "IPY_MODEL_f1e35c3bd0b94261b46ac1a099340794"
       ],
       "layout": "IPY_MODEL_e5f88cef16924c6daced44800d8bcfcd",
       "tabbable": null,
       "tooltip": null
      }
     },
     "54c0fbb05beb47ff9d15b9caf1d000bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "59005a407c63499b955332e0b139fa7a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5f7fd738283141618b705900a0ed18b9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5ff3cde55ea941d4a7315cc995b5ea9f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "634f0e77e5b34d5685f608806ab202b2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "64aab57ec23e4795aca104534757ba15": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "65815a61df3e4afab26b0ef5d9267211": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6592ec83f98b469dbd6a4d0b62e0a4c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "663cd064b5c14c97965e748726283143": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_0742dda7ce52495098e570e238e0e5b7",
        "IPY_MODEL_fe314363b3e84ed284e6cc957bc61b26",
        "IPY_MODEL_c0d5368af76b4a2d85c6659059ceaabd"
       ],
       "layout": "IPY_MODEL_10f619ea10154149ba58578d9fce7e77",
       "tabbable": null,
       "tooltip": null
      }
     },
     "68b69e358ff64e8bb68889a0c1d89f7d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6b8e25b2fb4844df81d7d2b39bade36e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "71f8bb596335412f82f0f8baf2602a37": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7583fba1552f4ad3b7fe4785afe1bdbe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7a690829f81c4839938df76e5b2aa9c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_71f8bb596335412f82f0f8baf2602a37",
       "placeholder": "​",
       "style": "IPY_MODEL_43977cc3a97247ec869c94dff6a8aece",
       "tabbable": null,
       "tooltip": null,
       "value": " 1.96M/1.96M [00:00&lt;00:00, 2.69MB/s]"
      }
     },
     "7de3f43a96564153a71c9cefead817a7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8084979977b2415082441f1263c48cb2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "88ad0a006558411eaf1280b8129ef6c3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8ee481f6ea894000803ee803847e96f9",
       "placeholder": "​",
       "style": "IPY_MODEL_a97a017310ec4454b501920c4f6b63af",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json: "
      }
     },
     "8e42b5d6d03b4139acd8522f2afc5f8e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8ee481f6ea894000803ee803847e96f9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8f5855c8994543c197965bef13ce5bea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "91449a798f664823947912d18e857703": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "980b469ccce94f658e399ae6bf996846": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "998944a2a7c34532aed40a1327c11ac7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9cf72d140fb04d09bcb0e51e30cdddcf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a607a51ba8ae46bd84c7907f166a1016": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a6aa787459b142c9971ce2f42be8f827": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a97a017310ec4454b501920c4f6b63af": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "abf43c660b034d85967121d73cbba718": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_88ad0a006558411eaf1280b8129ef6c3",
        "IPY_MODEL_492aceb73fe4478ca920faa87a0f7db3",
        "IPY_MODEL_f2c0fc3658bc47e2851da878a77147ae"
       ],
       "layout": "IPY_MODEL_dac6ea43090c44e38fcf2c73fec787a3",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b3ad9605ad1f4939b102ca94d9f5ed47": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cbda960cb13448faa0a84647599300db",
       "placeholder": "​",
       "style": "IPY_MODEL_980b469ccce94f658e399ae6bf996846",
       "tabbable": null,
       "tooltip": null,
       "value": " 49.0/49.0 [00:00&lt;00:00, 5.20kB/s]"
      }
     },
     "bbff34a848dc4ccc80b7ab54d216ba30": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c02537cc628b4acf8991f0d0baa6e031": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a607a51ba8ae46bd84c7907f166a1016",
       "placeholder": "​",
       "style": "IPY_MODEL_6b8e25b2fb4844df81d7d2b39bade36e",
       "tabbable": null,
       "tooltip": null,
       "value": " 996k/996k [00:00&lt;00:00, 1.85MB/s]"
      }
     },
     "c0d5368af76b4a2d85c6659059ceaabd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_458d2337a752418888d532a2bda15c88",
       "placeholder": "​",
       "style": "IPY_MODEL_d964dda636984a2b9bef2af68e2f46d2",
       "tabbable": null,
       "tooltip": null,
       "value": " 466/466 [00:00&lt;00:00, 56.1kB/s]"
      }
     },
     "c4229f32cf8e485382702c0c57c253be": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d2f36c8e967b46e8bc5582caa54da835",
       "placeholder": "​",
       "style": "IPY_MODEL_0d319f30536642eeab09ee599ac10d74",
       "tabbable": null,
       "tooltip": null,
       "value": "vocab.txt: 100%"
      }
     },
     "c6e91bef3ff94475b8d7025dee17ac2b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c82b5c4f065749e4861b7eee17fb3b68": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cbda960cb13448faa0a84647599300db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d2f36c8e967b46e8bc5582caa54da835": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d964dda636984a2b9bef2af68e2f46d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "da1026b655aa4892a0466a00e0066f54": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_21f548adaba844178d8eaf0a4cc0ec76",
       "max": 49,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_68b69e358ff64e8bb68889a0c1d89f7d",
       "tabbable": null,
       "tooltip": null,
       "value": 49
      }
     },
     "da682b13c34e479ca215e213f48aaea1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_314db1afbda148a5813b016b8b8431f3",
       "max": 541795680,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8f5855c8994543c197965bef13ce5bea",
       "tabbable": null,
       "tooltip": null,
       "value": 541795680
      }
     },
     "dac6ea43090c44e38fcf2c73fec787a3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e3878f0da90d44d58533edfceaa72763": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e3a00f157df54b44b29a0b1f98254da3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7de3f43a96564153a71c9cefead817a7",
       "placeholder": "​",
       "style": "IPY_MODEL_5ff3cde55ea941d4a7315cc995b5ea9f",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer_config.json: 100%"
      }
     },
     "e5f88cef16924c6daced44800d8bcfcd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e69d5b410de346d4add584e3786c3ab5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e739366c152041eab509fe88c9ba8fc9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e89aaac635f440fbbf35e8011db0d0fa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "eb0f02e7f7ca4a4986241017f80e6d00": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c6e91bef3ff94475b8d7025dee17ac2b",
       "placeholder": "​",
       "style": "IPY_MODEL_4e052da5e91c4a5684a04f26d69026ed",
       "tabbable": null,
       "tooltip": null,
       "value": " 542M/542M [00:06&lt;00:00, 69.1MB/s]"
      }
     },
     "ef893dd577784626b068e6896b99e440": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_2cb21c81c9d94b6f8494830cc710132c",
        "IPY_MODEL_da682b13c34e479ca215e213f48aaea1",
        "IPY_MODEL_eb0f02e7f7ca4a4986241017f80e6d00"
       ],
       "layout": "IPY_MODEL_fc2f4ef2e0eb455d809971eb0d5f8ee7",
       "tabbable": null,
       "tooltip": null
      }
     },
     "efe483f860ab48019aca5f927bc82970": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1fd43d4e688e41db8bd37ed519702c02",
        "IPY_MODEL_28dc9b4b2221457ab528143ac162fea3",
        "IPY_MODEL_7a690829f81c4839938df76e5b2aa9c6"
       ],
       "layout": "IPY_MODEL_91449a798f664823947912d18e857703",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f1e35c3bd0b94261b46ac1a099340794": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_634f0e77e5b34d5685f608806ab202b2",
       "placeholder": "​",
       "style": "IPY_MODEL_998944a2a7c34532aed40a1327c11ac7",
       "tabbable": null,
       "tooltip": null,
       "value": " 486M/486M [00:03&lt;00:00, 113MB/s]"
      }
     },
     "f2c0fc3658bc47e2851da878a77147ae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e69d5b410de346d4add584e3786c3ab5",
       "placeholder": "​",
       "style": "IPY_MODEL_fae4d39d573f4156a24a07cb76026496",
       "tabbable": null,
       "tooltip": null,
       "value": " 22.7k/? [00:00&lt;00:00, 2.33MB/s]"
      }
     },
     "f3d5c7f3c378450eb27ef595163aa00b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e3a00f157df54b44b29a0b1f98254da3",
        "IPY_MODEL_da1026b655aa4892a0466a00e0066f54",
        "IPY_MODEL_b3ad9605ad1f4939b102ca94d9f5ed47"
       ],
       "layout": "IPY_MODEL_3c2d589d0ee947a2994a00424b9e3335",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f5f46cf442fd43969493fad397cc7a11": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f715c94b6a1c4adfb0db01864134bd68": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f9ef44b3b69b4736a865195cbd7a7d84": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fae4d39d573f4156a24a07cb76026496": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fc2f4ef2e0eb455d809971eb0d5f8ee7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fe314363b3e84ed284e6cc957bc61b26": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7583fba1552f4ad3b7fe4785afe1bdbe",
       "max": 466,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_54c0fbb05beb47ff9d15b9caf1d000bb",
       "tabbable": null,
       "tooltip": null,
       "value": 466
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
